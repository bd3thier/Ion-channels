{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ion channel challenge: stratified split, SMOTE, 139 features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Conv1D, Dense, LSTM, BatchNormalization, Activation, Dropout, Lambda, \n",
    "                                     Multiply, Add, Concatenate, Flatten, MaxPooling1D)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Inspired by https://www.kaggle.com/siavrez/wavenet-keras?scriptVersionId=31418572\n",
    "def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n",
    "\n",
    "    def build_residual_block(l_input):\n",
    "        resid_input = l_input\n",
    "        for dilation_rate in [2**i for i in range(stacked_layer)]:\n",
    "            l_sigmoid_conv1d = Conv1D(\n",
    "              num_filters, kernel_size, dilation_rate=dilation_rate,\n",
    "              padding='same', activation='sigmoid')(l_input)\n",
    "            l_tanh_conv1d = Conv1D(\n",
    "             num_filters, kernel_size, dilation_rate=dilation_rate,\n",
    "             padding='same', activation='mish')(l_input)\n",
    "            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n",
    "            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n",
    "            resid_input = Add()([resid_input ,l_input])\n",
    "        return resid_input\n",
    "    return build_residual_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from https://www.kaggle.com/siavrez/wavenet-keras?scriptVersionId=31418572\n",
    "\n",
    "def mish(x):\n",
    "    y = x*K.tanh(K.softplus(x))\n",
    "    return y\n",
    "\n",
    "get_custom_objects().update({'mish': mish})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_array(df):\n",
    "    df_temp = df.to_numpy()\n",
    "    df= np.expand_dims(df_temp, axis=0)\n",
    "    df = df[ ..., np.newaxis]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df: pd.DataFrame,\n",
    "                     verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "\n",
    "                if (c_min > np.iinfo(np.int32).min\n",
    "                      and c_max < np.iinfo(np.int32).max):\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif (c_min > np.iinfo(np.int64).min\n",
    "                      and c_max < np.iinfo(np.int64).max):\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (c_min > np.finfo(np.float16).min\n",
    "                        and c_max < np.finfo(np.float16).max):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (c_min > np.finfo(np.float32).min\n",
    "                      and c_max < np.finfo(np.float32).max):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    reduction = (start_mem - end_mem) / start_mem\n",
    "\n",
    "    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_short(epoch):\n",
    "    if epoch < 30:    \n",
    "        return 1e-3\n",
    "    elif epoch >= 30 and epoch < 60:\n",
    "        return 5e-4\n",
    "    elif epoch >= 60 and epoch < 120:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 5e-5\n",
    "\n",
    "def decay_long(epoch):\n",
    "    if epoch < 100:\n",
    "        return 1e-3\n",
    "    elif epoch >= 100 and epoch < 200:\n",
    "        return 5e-4\n",
    "    elif epoch >= 200 and epoch < 500:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal with kalman filter was downloaded from [michaln](https://www.kaggle.com/michaln/data-without-drift-with-kalman-filter).\n",
    "We added features in three notebooks ([here](https://github.com/berenice-d/Ion-channels/blob/master/notebooks/Ion%20channel%20-%20Feature%20Engineering%201.ipynb), [here](https://github.com/berenice-d/Ion-channels/blob/master/notebooks/Ion%20channel%20-%20Feature%20engineering%202.ipynb) and [here](https://github.com/berenice-d/Ion-channels/blob/master/notebooks/Ion%20channel%20-%20Feature%20engineering%203.ipynb)), and Catboost predictions in [this notebook](https://github.com/berenice-d/Ion-channels/blob/master/notebooks/Ion%20channel%20-%20Catboost%20key%20models.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and setup data\n",
    "See [this notebook](https://github.com/berenice-d/Ion-channels/blob/master/notebooks/Ion%20channel%20-%20Catboost%20key%20models.ipynb) and [Deep channel GitHub](https://github.com/RichardBJ/Deep-Channel/blob/master/deepchannel_train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('../data/interim/train_2.1.7.1catpred.csv')#.sample(frac=1, random_state=134)\n",
    "test = pd.read_csv('../data/interim/test_2.1.7.1catpred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.open_channels\n",
    "X = train.drop(['open_channels'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/kmat2019/u-net-1d-cnn-with-keras\n",
    "# I don't use \"time\" feature\n",
    "X = X.values.reshape(-1,5000,X.shape[1])#number_of_data:1250 x time_step:4000\n",
    "\n",
    "\n",
    "X_test = test.values.reshape(-1, 5000,test.shape[1])#\n",
    "\n",
    "y = to_categorical(y).reshape(-1,5000,11)#classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(800, 5000, 6), X_validation:(200, 5000, 6), y_train:(800, 5000, 11), y_validation:(200, 5000, 11)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8, random_state=134)\n",
    "print(\"X_train:{}, X_validation:{}, y_train:{}, y_validation:{}\".format(X_train.shape, X_validation.shape, \n",
    "                                                                        y_train.shape, y_validation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet model 92\n",
    "\n",
    "This last model was saved as 2.2.6_11. \n",
    "\n",
    "LB score:0.94 F1 score:0.95661\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_ = 32 \n",
    "kernel_size_ = 2\n",
    "verbose, epochs, batch_size = 1, 500, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[2]\n",
    "stacked_layers_ = [8, 4, 2, 1]\n",
    "input_shape = (n_timesteps, n_features)\n",
    "LR = 0.0001\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=100)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "lrs = LearningRateScheduler(decay_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input = Input(shape=(input_shape))\n",
    "x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
    "#, bias_regularizer=l2(0.01), kernel_regularizer=l2(0.01)\n",
    "x = BatchNormalization()(x)\n",
    "x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
    "x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
    "x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
    "x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = Dense(1500, activation='relu')(x)\n",
    "l_output = Dense(11, activation='softmax')(x)\n",
    "model = models.Model(inputs=[l_input], outputs=[l_output])\n",
    "opt = Adam(lr=LR)\n",
    "opt = tfa.optimizers.SWA(opt)\n",
    "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5000, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 5000, 32)     224         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 5000, 32)     128         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5000, 32)     2080        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 5000, 32)     2080        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 5000, 32)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 5000, 32)     1056        multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5000, 32)     2080        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 5000, 32)     2080        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 5000, 32)     0           conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 5000, 32)     1056        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 5000, 32)     2080        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 5000, 32)     2080        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 5000, 32)     0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 5000, 32)     1056        multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 5000, 32)     2080        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 5000, 32)     2080        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 5000, 32)     0           conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 5000, 32)     1056        multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 5000, 32)     2080        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 5000, 32)     2080        conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 5000, 32)     0           conv1d_13[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 5000, 32)     1056        multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 5000, 32)     2080        conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 5000, 32)     2080        conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 5000, 32)     0           conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 5000, 32)     0           batch_normalization[0][0]        \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 5000, 32)     1056        multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 5000, 32)     0           add[0][0]                        \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 5000, 32)     2080        conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 5000, 32)     2080        conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 5000, 32)     0           add_1[0][0]                      \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 5000, 32)     0           conv1d_19[0][0]                  \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 5000, 32)     0           add_2[0][0]                      \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 5000, 32)     1056        multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 5000, 32)     0           add_3[0][0]                      \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 5000, 32)     2080        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 5000, 32)     2080        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 5000, 32)     0           add_4[0][0]                      \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 5000, 32)     0           conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 5000, 32)     0           add_5[0][0]                      \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 5000, 32)     1056        multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 5000, 32)     0           add_6[0][0]                      \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 5000, 64)     2112        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 5000, 64)     8256        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 5000, 64)     8256        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 5000, 64)     0           conv1d_26[0][0]                  \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 5000, 64)     4160        multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 5000, 64)     8256        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 5000, 64)     8256        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 5000, 64)     0           conv1d_29[0][0]                  \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 5000, 64)     4160        multiply_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 5000, 64)     8256        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 5000, 64)     8256        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 5000, 64)     0           conv1d_32[0][0]                  \n",
      "                                                                 conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 5000, 64)     4160        multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 5000, 64)     0           conv1d_25[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 5000, 64)     8256        conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 5000, 64)     8256        conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 5000, 64)     0           add_8[0][0]                      \n",
      "                                                                 conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 5000, 64)     0           conv1d_35[0][0]                  \n",
      "                                                                 conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 5000, 64)     0           add_9[0][0]                      \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 5000, 64)     4160        multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 5000, 64)     0           add_10[0][0]                     \n",
      "                                                                 conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 5000, 128)    8320        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 5000, 128)    32896       conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 5000, 128)    32896       conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 5000, 128)    0           conv1d_39[0][0]                  \n",
      "                                                                 conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 5000, 128)    16512       multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 5000, 128)    32896       conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 5000, 128)    32896       conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 5000, 128)    0           conv1d_42[0][0]                  \n",
      "                                                                 conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 5000, 128)    0           conv1d_38[0][0]                  \n",
      "                                                                 conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 5000, 128)    16512       multiply_13[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 5000, 128)    0           add_12[0][0]                     \n",
      "                                                                 conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 5000, 256)    33024       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 5000, 256)    131328      conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 5000, 256)    131328      conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 5000, 256)    0           conv1d_46[0][0]                  \n",
      "                                                                 conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 5000, 256)    65792       multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 5000, 256)    0           conv1d_45[0][0]                  \n",
      "                                                                 conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5000, 1500)   385500      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5000, 11)     16511       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,063,291\n",
      "Trainable params: 1,063,227\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:From C:\\Users\\beren\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.4046 - accuracy: 0.4677\n",
      "Epoch 00001: val_loss improved from inf to 6.88316, saving model to best_model.h5\n",
      "25/25 [==============================] - 9s 350ms/step - loss: 1.4046 - accuracy: 0.4677 - val_loss: 6.8832 - val_accuracy: 0.2823 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.8684 - accuracy: 0.6693\n",
      "Epoch 00002: val_loss improved from 6.88316 to 3.34750, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.8684 - accuracy: 0.6693 - val_loss: 3.3475 - val_accuracy: 0.3204 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.7748\n",
      "Epoch 00003: val_loss improved from 3.34750 to 2.34155, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.6098 - accuracy: 0.7748 - val_loss: 2.3415 - val_accuracy: 0.4609 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4774 - accuracy: 0.8198\n",
      "Epoch 00004: val_loss improved from 2.34155 to 1.85217, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.4774 - accuracy: 0.8198 - val_loss: 1.8522 - val_accuracy: 0.4637 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3898 - accuracy: 0.8471\n",
      "Epoch 00005: val_loss did not improve from 1.85217\n",
      "25/25 [==============================] - 7s 290ms/step - loss: 0.3898 - accuracy: 0.8471 - val_loss: 2.2446 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8535\n",
      "Epoch 00006: val_loss improved from 1.85217 to 1.62596, saving model to best_model.h5\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.4182 - accuracy: 0.8535 - val_loss: 1.6260 - val_accuracy: 0.5001 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.8508\n",
      "Epoch 00007: val_loss did not improve from 1.62596\n",
      "25/25 [==============================] - 7s 290ms/step - loss: 0.4128 - accuracy: 0.8508 - val_loss: 1.9299 - val_accuracy: 0.4952 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8844\n",
      "Epoch 00008: val_loss did not improve from 1.62596\n",
      "25/25 [==============================] - 7s 291ms/step - loss: 0.3118 - accuracy: 0.8844 - val_loss: 2.4608 - val_accuracy: 0.4936 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8860\n",
      "Epoch 00009: val_loss did not improve from 1.62596\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.3097 - accuracy: 0.8860 - val_loss: 2.3122 - val_accuracy: 0.4888 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2066 - accuracy: 0.9301\n",
      "Epoch 00010: val_loss did not improve from 1.62596\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.2066 - accuracy: 0.9301 - val_loss: 3.9903 - val_accuracy: 0.4594 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1937 - accuracy: 0.9342\n",
      "Epoch 00011: val_loss improved from 1.62596 to 1.23133, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 315ms/step - loss: 0.1937 - accuracy: 0.9342 - val_loss: 1.2313 - val_accuracy: 0.6342 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9176\n",
      "Epoch 00012: val_loss did not improve from 1.23133\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.2220 - accuracy: 0.9176 - val_loss: 2.7611 - val_accuracy: 0.5024 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9267\n",
      "Epoch 00013: val_loss did not improve from 1.23133\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.2186 - accuracy: 0.9267 - val_loss: 1.6535 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1645 - accuracy: 0.9435\n",
      "Epoch 00014: val_loss did not improve from 1.23133\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.1645 - accuracy: 0.9435 - val_loss: 1.8589 - val_accuracy: 0.6155 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.9100\n",
      "Epoch 00015: val_loss improved from 1.23133 to 0.71173, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.2558 - accuracy: 0.9100 - val_loss: 0.7117 - val_accuracy: 0.7826 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.8995\n",
      "Epoch 00016: val_loss improved from 0.71173 to 0.42838, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.2730 - accuracy: 0.8995 - val_loss: 0.4284 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9178\n",
      "Epoch 00017: val_loss did not improve from 0.42838\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.2250 - accuracy: 0.9178 - val_loss: 0.7693 - val_accuracy: 0.8139 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9391\n",
      "Epoch 00018: val_loss did not improve from 0.42838\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.1701 - accuracy: 0.9391 - val_loss: 1.5738 - val_accuracy: 0.7068 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9206\n",
      "Epoch 00019: val_loss did not improve from 0.42838\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.2173 - accuracy: 0.9206 - val_loss: 1.1276 - val_accuracy: 0.7309 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9428\n",
      "Epoch 00020: val_loss did not improve from 0.42838\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.1672 - accuracy: 0.9428 - val_loss: 0.4554 - val_accuracy: 0.8099 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9532\n",
      "Epoch 00021: val_loss did not improve from 0.42838\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.1443 - accuracy: 0.9532 - val_loss: 0.8595 - val_accuracy: 0.7677 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9646\n",
      "Epoch 00022: val_loss did not improve from 0.42838\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.1139 - accuracy: 0.9646 - val_loss: 0.5975 - val_accuracy: 0.8119 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9609\n",
      "Epoch 00023: val_loss improved from 0.42838 to 0.15596, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.1211 - accuracy: 0.9609 - val_loss: 0.1560 - val_accuracy: 0.9346 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9470\n",
      "Epoch 00024: val_loss did not improve from 0.15596\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1610 - accuracy: 0.9470 - val_loss: 0.4126 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9520\n",
      "Epoch 00025: val_loss improved from 0.15596 to 0.14961, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.1445 - accuracy: 0.9520 - val_loss: 0.1496 - val_accuracy: 0.9445 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9611\n",
      "Epoch 00026: val_loss improved from 0.14961 to 0.11025, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1221 - accuracy: 0.9611 - val_loss: 0.1102 - val_accuracy: 0.9681 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9507\n",
      "Epoch 00027: val_loss improved from 0.11025 to 0.10011, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1479 - accuracy: 0.9507 - val_loss: 0.1001 - val_accuracy: 0.9711 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9617\n",
      "Epoch 00028: val_loss did not improve from 0.10011\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.1256 - accuracy: 0.9617 - val_loss: 0.1044 - val_accuracy: 0.9696 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9620\n",
      "Epoch 00029: val_loss did not improve from 0.10011\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.1200 - accuracy: 0.9620 - val_loss: 0.1059 - val_accuracy: 0.9688 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9652\n",
      "Epoch 00030: val_loss did not improve from 0.10011\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.1138 - accuracy: 0.9652 - val_loss: 0.1031 - val_accuracy: 0.9698 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9602\n",
      "Epoch 00031: val_loss did not improve from 0.10011\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1249 - accuracy: 0.9602 - val_loss: 0.1040 - val_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9650\n",
      "Epoch 00032: val_loss did not improve from 0.10011\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.1130 - accuracy: 0.9650 - val_loss: 0.1076 - val_accuracy: 0.9687 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9644\n",
      "Epoch 00033: val_loss improved from 0.10011 to 0.09248, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.1158 - accuracy: 0.9644 - val_loss: 0.0925 - val_accuracy: 0.9732 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9665\n",
      "Epoch 00034: val_loss did not improve from 0.09248\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.1127 - accuracy: 0.9665 - val_loss: 0.3766 - val_accuracy: 0.8465 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9433\n",
      "Epoch 00035: val_loss did not improve from 0.09248\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.1897 - accuracy: 0.9433 - val_loss: 0.2009 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9456\n",
      "Epoch 00036: val_loss did not improve from 0.09248\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.1598 - accuracy: 0.9456 - val_loss: 0.4004 - val_accuracy: 0.8525 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9406\n",
      "Epoch 00037: val_loss did not improve from 0.09248\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.1731 - accuracy: 0.9406 - val_loss: 0.1190 - val_accuracy: 0.9659 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9254\n",
      "Epoch 00038: val_loss did not improve from 0.09248\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.2064 - accuracy: 0.9254 - val_loss: 0.1420 - val_accuracy: 0.9518 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9648\n",
      "Epoch 00039: val_loss improved from 0.09248 to 0.09172, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.1151 - accuracy: 0.9648 - val_loss: 0.0917 - val_accuracy: 0.9732 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1006 - accuracy: 0.9698\n",
      "Epoch 00040: val_loss improved from 0.09172 to 0.08713, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.1006 - accuracy: 0.9698 - val_loss: 0.0871 - val_accuracy: 0.9741 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9709\n",
      "Epoch 00041: val_loss did not improve from 0.08713\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0975 - accuracy: 0.9709 - val_loss: 0.0891 - val_accuracy: 0.9733 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9698\n",
      "Epoch 00042: val_loss did not improve from 0.08713\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0993 - accuracy: 0.9698 - val_loss: 0.0882 - val_accuracy: 0.9738 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9712\n",
      "Epoch 00043: val_loss improved from 0.08713 to 0.08518, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0970 - accuracy: 0.9712 - val_loss: 0.0852 - val_accuracy: 0.9746 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9694\n",
      "Epoch 00044: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1018 - accuracy: 0.9694 - val_loss: 0.0871 - val_accuracy: 0.9744 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1040 - accuracy: 0.9685\n",
      "Epoch 00045: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.1040 - accuracy: 0.9685 - val_loss: 0.3690 - val_accuracy: 0.8620 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9632\n",
      "Epoch 00046: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.1175 - accuracy: 0.9632 - val_loss: 0.2581 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9592\n",
      "Epoch 00047: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.1334 - accuracy: 0.9592 - val_loss: 0.1145 - val_accuracy: 0.9681 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9714\n",
      "Epoch 00048: val_loss improved from 0.08518 to 0.08518, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0971 - accuracy: 0.9714 - val_loss: 0.0852 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1082 - accuracy: 0.9670\n",
      "Epoch 00049: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1082 - accuracy: 0.9670 - val_loss: 0.1772 - val_accuracy: 0.9412 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9680\n",
      "Epoch 00050: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.1068 - accuracy: 0.9680 - val_loss: 0.0912 - val_accuracy: 0.9746 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9700\n",
      "Epoch 00051: val_loss did not improve from 0.08518\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0997 - accuracy: 0.9700 - val_loss: 0.0948 - val_accuracy: 0.9732 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9719\n",
      "Epoch 00052: val_loss improved from 0.08518 to 0.08439, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0950 - accuracy: 0.9719 - val_loss: 0.0844 - val_accuracy: 0.9755 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9714\n",
      "Epoch 00053: val_loss did not improve from 0.08439\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0975 - accuracy: 0.9714 - val_loss: 0.0847 - val_accuracy: 0.9754 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9674\n",
      "Epoch 00054: val_loss did not improve from 0.08439\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.0925 - val_accuracy: 0.9738 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9676\n",
      "Epoch 00055: val_loss did not improve from 0.08439\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 0.1091 - accuracy: 0.9676 - val_loss: 0.0910 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9727\n",
      "Epoch 00056: val_loss did not improve from 0.08439\n",
      "25/25 [==============================] - 9s 350ms/step - loss: 0.0944 - accuracy: 0.9727 - val_loss: 0.0874 - val_accuracy: 0.9749 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9700\n",
      "Epoch 00057: val_loss did not improve from 0.08439\n",
      "25/25 [==============================] - 10s 386ms/step - loss: 0.0997 - accuracy: 0.9700 - val_loss: 0.1716 - val_accuracy: 0.9355 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9716\n",
      "Epoch 00058: val_loss did not improve from 0.08439\n",
      "25/25 [==============================] - 9s 373ms/step - loss: 0.0967 - accuracy: 0.9716 - val_loss: 0.0866 - val_accuracy: 0.9747 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9728\n",
      "Epoch 00059: val_loss improved from 0.08439 to 0.08118, saving model to best_model.h5\n",
      "25/25 [==============================] - 9s 349ms/step - loss: 0.0920 - accuracy: 0.9728 - val_loss: 0.0812 - val_accuracy: 0.9759 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9519\n",
      "Epoch 00060: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 9s 343ms/step - loss: 0.1470 - accuracy: 0.9519 - val_loss: 0.1168 - val_accuracy: 0.9642 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9207\n",
      "Epoch 00061: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 328ms/step - loss: 0.2211 - accuracy: 0.9207 - val_loss: 0.1135 - val_accuracy: 0.9648 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9463\n",
      "Epoch 00062: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1586 - accuracy: 0.9463 - val_loss: 0.1307 - val_accuracy: 0.9624 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9604\n",
      "Epoch 00063: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.1256 - accuracy: 0.9604 - val_loss: 0.0956 - val_accuracy: 0.9729 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9711\n",
      "Epoch 00064: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0992 - accuracy: 0.9711 - val_loss: 0.0862 - val_accuracy: 0.9751 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9721\n",
      "Epoch 00065: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.0953 - accuracy: 0.9721 - val_loss: 0.1280 - val_accuracy: 0.9609 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9736\n",
      "Epoch 00066: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.0911 - accuracy: 0.9736 - val_loss: 0.0842 - val_accuracy: 0.9759 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9739\n",
      "Epoch 00067: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 0.0884 - accuracy: 0.9739 - val_loss: 0.0839 - val_accuracy: 0.9755 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9649\n",
      "Epoch 00068: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.1109 - accuracy: 0.9649 - val_loss: 0.1111 - val_accuracy: 0.9652 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9658\n",
      "Epoch 00069: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1134 - accuracy: 0.9658 - val_loss: 0.0866 - val_accuracy: 0.9757 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9627\n",
      "Epoch 00070: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.1260 - accuracy: 0.9627 - val_loss: 0.1176 - val_accuracy: 0.9635 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9647\n",
      "Epoch 00071: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.1169 - accuracy: 0.9647 - val_loss: 0.1300 - val_accuracy: 0.9621 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9715\n",
      "Epoch 00072: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.1002 - accuracy: 0.9715 - val_loss: 0.0862 - val_accuracy: 0.9755 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9734\n",
      "Epoch 00073: val_loss did not improve from 0.08118\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0936 - accuracy: 0.9734 - val_loss: 0.0815 - val_accuracy: 0.9760 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9727\n",
      "Epoch 00074: val_loss improved from 0.08118 to 0.08017, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.0930 - accuracy: 0.9727 - val_loss: 0.0802 - val_accuracy: 0.9763 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9741\n",
      "Epoch 00075: val_loss did not improve from 0.08017\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.0886 - accuracy: 0.9741 - val_loss: 0.0803 - val_accuracy: 0.9764 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9741\n",
      "Epoch 00076: val_loss did not improve from 0.08017\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0875 - accuracy: 0.9741 - val_loss: 0.0813 - val_accuracy: 0.9754 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9747\n",
      "Epoch 00077: val_loss improved from 0.08017 to 0.07899, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.0854 - accuracy: 0.9747 - val_loss: 0.0790 - val_accuracy: 0.9765 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9738\n",
      "Epoch 00078: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0873 - accuracy: 0.9738 - val_loss: 0.0811 - val_accuracy: 0.9757 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9727\n",
      "Epoch 00079: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.0906 - accuracy: 0.9727 - val_loss: 0.0952 - val_accuracy: 0.9733 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9712\n",
      "Epoch 00080: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 0.0943 - accuracy: 0.9712 - val_loss: 0.0975 - val_accuracy: 0.9716 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9660\n",
      "Epoch 00081: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.1065 - accuracy: 0.9660 - val_loss: 0.1279 - val_accuracy: 0.9576 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9607\n",
      "Epoch 00082: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1263 - accuracy: 0.9607 - val_loss: 0.0957 - val_accuracy: 0.9713 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9635\n",
      "Epoch 00083: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.1166 - accuracy: 0.9635 - val_loss: 0.1578 - val_accuracy: 0.9523 - lr: 0.0010\n",
      "Epoch 84/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9709\n",
      "Epoch 00084: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0981 - accuracy: 0.9709 - val_loss: 0.1243 - val_accuracy: 0.9637 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9734\n",
      "Epoch 00085: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0896 - accuracy: 0.9734 - val_loss: 0.0828 - val_accuracy: 0.9750 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9730\n",
      "Epoch 00086: val_loss did not improve from 0.07899\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0907 - accuracy: 0.9730 - val_loss: 0.0839 - val_accuracy: 0.9760 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9752\n",
      "Epoch 00087: val_loss improved from 0.07899 to 0.07771, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0833 - accuracy: 0.9752 - val_loss: 0.0777 - val_accuracy: 0.9766 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9756\n",
      "Epoch 00088: val_loss did not improve from 0.07771\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0821 - accuracy: 0.9756 - val_loss: 0.0787 - val_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9754\n",
      "Epoch 00089: val_loss improved from 0.07771 to 0.07706, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0826 - accuracy: 0.9754 - val_loss: 0.0771 - val_accuracy: 0.9763 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9751\n",
      "Epoch 00090: val_loss did not improve from 0.07706\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.0824 - accuracy: 0.9751 - val_loss: 0.0792 - val_accuracy: 0.9763 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9757\n",
      "Epoch 00091: val_loss improved from 0.07706 to 0.07615, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 326ms/step - loss: 0.0814 - accuracy: 0.9757 - val_loss: 0.0761 - val_accuracy: 0.9768 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9746\n",
      "Epoch 00092: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 323ms/step - loss: 0.0845 - accuracy: 0.9746 - val_loss: 0.0767 - val_accuracy: 0.9769 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9745\n",
      "Epoch 00093: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.0845 - accuracy: 0.9745 - val_loss: 0.0786 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9726\n",
      "Epoch 00094: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.0900 - accuracy: 0.9726 - val_loss: 0.0820 - val_accuracy: 0.9761 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9747\n",
      "Epoch 00095: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0850 - accuracy: 0.9747 - val_loss: 0.0775 - val_accuracy: 0.9765 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9753\n",
      "Epoch 00096: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0834 - accuracy: 0.9753 - val_loss: 0.0763 - val_accuracy: 0.9769 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9698\n",
      "Epoch 00097: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0998 - accuracy: 0.9698 - val_loss: 0.0838 - val_accuracy: 0.9758 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9715\n",
      "Epoch 00098: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.0946 - accuracy: 0.9715 - val_loss: 0.0880 - val_accuracy: 0.9742 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9755\n",
      "Epoch 00099: val_loss did not improve from 0.07615\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.0839 - accuracy: 0.9755 - val_loss: 0.0786 - val_accuracy: 0.9769 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9763\n",
      "Epoch 00100: val_loss improved from 0.07615 to 0.07575, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.0794 - accuracy: 0.9763 - val_loss: 0.0758 - val_accuracy: 0.9767 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9763\n",
      "Epoch 00101: val_loss improved from 0.07575 to 0.07510, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 323ms/step - loss: 0.0781 - accuracy: 0.9763 - val_loss: 0.0751 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9764\n",
      "Epoch 00102: val_loss improved from 0.07510 to 0.07433, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0780 - accuracy: 0.9764 - val_loss: 0.0743 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9766\n",
      "Epoch 00103: val_loss improved from 0.07433 to 0.07430, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.0774 - accuracy: 0.9766 - val_loss: 0.0743 - val_accuracy: 0.9768 - lr: 5.0000e-04\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9763\n",
      "Epoch 00104: val_loss improved from 0.07430 to 0.07401, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.0785 - accuracy: 0.9763 - val_loss: 0.0740 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9764\n",
      "Epoch 00105: val_loss did not improve from 0.07401\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0783 - accuracy: 0.9764 - val_loss: 0.0756 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9765\n",
      "Epoch 00106: val_loss did not improve from 0.07401\n",
      "25/25 [==============================] - 8s 316ms/step - loss: 0.0779 - accuracy: 0.9765 - val_loss: 0.0742 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9765\n",
      "Epoch 00107: val_loss did not improve from 0.07401\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 0.0775 - accuracy: 0.9765 - val_loss: 0.0748 - val_accuracy: 0.9767 - lr: 5.0000e-04\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9763\n",
      "Epoch 00108: val_loss did not improve from 0.07401\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0792 - accuracy: 0.9763 - val_loss: 0.0746 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9768\n",
      "Epoch 00109: val_loss improved from 0.07401 to 0.07363, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.0765 - accuracy: 0.9768 - val_loss: 0.0736 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9767\n",
      "Epoch 00110: val_loss did not improve from 0.07363\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.0763 - accuracy: 0.9767 - val_loss: 0.0736 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9743\n",
      "Epoch 00111: val_loss did not improve from 0.07363\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.0841 - accuracy: 0.9743 - val_loss: 0.0763 - val_accuracy: 0.9766 - lr: 5.0000e-04\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9755\n",
      "Epoch 00112: val_loss did not improve from 0.07363\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.0806 - accuracy: 0.9755 - val_loss: 0.0741 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9766\n",
      "Epoch 00113: val_loss improved from 0.07363 to 0.07341, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 328ms/step - loss: 0.0767 - accuracy: 0.9766 - val_loss: 0.0734 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9767\n",
      "Epoch 00114: val_loss improved from 0.07341 to 0.07313, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 327ms/step - loss: 0.0766 - accuracy: 0.9767 - val_loss: 0.0731 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9767\n",
      "Epoch 00115: val_loss did not improve from 0.07313\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 0.0764 - accuracy: 0.9767 - val_loss: 0.0734 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9761\n",
      "Epoch 00116: val_loss did not improve from 0.07313\n",
      "25/25 [==============================] - 8s 316ms/step - loss: 0.0785 - accuracy: 0.9761 - val_loss: 0.0740 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9765\n",
      "Epoch 00117: val_loss did not improve from 0.07313\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 0.0770 - accuracy: 0.9765 - val_loss: 0.0740 - val_accuracy: 0.9768 - lr: 5.0000e-04\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9765\n",
      "Epoch 00118: val_loss did not improve from 0.07313\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 0.0769 - accuracy: 0.9765 - val_loss: 0.0732 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9770\n",
      "Epoch 00119: val_loss improved from 0.07313 to 0.07302, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 333ms/step - loss: 0.0754 - accuracy: 0.9770 - val_loss: 0.0730 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9768\n",
      "Epoch 00120: val_loss did not improve from 0.07302\n",
      "25/25 [==============================] - 8s 318ms/step - loss: 0.0763 - accuracy: 0.9768 - val_loss: 0.0732 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9768\n",
      "Epoch 00121: val_loss did not improve from 0.07302\n",
      "25/25 [==============================] - 8s 318ms/step - loss: 0.0757 - accuracy: 0.9768 - val_loss: 0.0731 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9763\n",
      "Epoch 00122: val_loss did not improve from 0.07302\n",
      "25/25 [==============================] - 8s 320ms/step - loss: 0.0770 - accuracy: 0.9763 - val_loss: 0.0731 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9769\n",
      "Epoch 00123: val_loss improved from 0.07302 to 0.07266, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 326ms/step - loss: 0.0754 - accuracy: 0.9769 - val_loss: 0.0727 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9762\n",
      "Epoch 00124: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.0781 - accuracy: 0.9762 - val_loss: 0.0743 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9746\n",
      "Epoch 00125: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0830 - accuracy: 0.9746 - val_loss: 0.2712 - val_accuracy: 0.8964 - lr: 5.0000e-04\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9733\n",
      "Epoch 00126: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0893 - accuracy: 0.9733 - val_loss: 0.0765 - val_accuracy: 0.9767 - lr: 5.0000e-04\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9760\n",
      "Epoch 00127: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0784 - accuracy: 0.9760 - val_loss: 0.0738 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9766\n",
      "Epoch 00128: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0765 - accuracy: 0.9766 - val_loss: 0.0733 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9767\n",
      "Epoch 00129: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0760 - accuracy: 0.9767 - val_loss: 0.0733 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9768\n",
      "Epoch 00130: val_loss did not improve from 0.07266\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0761 - accuracy: 0.9768 - val_loss: 0.0732 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9769\n",
      "Epoch 00131: val_loss improved from 0.07266 to 0.07264, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.0749 - accuracy: 0.9769 - val_loss: 0.0726 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9768\n",
      "Epoch 00132: val_loss did not improve from 0.07264\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.0755 - accuracy: 0.9768 - val_loss: 0.0727 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9770\n",
      "Epoch 00133: val_loss improved from 0.07264 to 0.07263, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 320ms/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.0726 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9770\n",
      "Epoch 00134: val_loss improved from 0.07263 to 0.07248, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 0.0748 - accuracy: 0.9770 - val_loss: 0.0725 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9762\n",
      "Epoch 00135: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 0.0775 - accuracy: 0.9762 - val_loss: 0.0756 - val_accuracy: 0.9771 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9732\n",
      "Epoch 00136: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.0870 - accuracy: 0.9732 - val_loss: 0.0814 - val_accuracy: 0.9763 - lr: 5.0000e-04\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9752\n",
      "Epoch 00137: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.0836 - accuracy: 0.9752 - val_loss: 0.0746 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9767\n",
      "Epoch 00138: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 0.0760 - accuracy: 0.9767 - val_loss: 0.0729 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 139/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9767\n",
      "Epoch 00139: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0757 - accuracy: 0.9767 - val_loss: 0.0728 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9767\n",
      "Epoch 00140: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0759 - accuracy: 0.9767 - val_loss: 0.0735 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9766\n",
      "Epoch 00141: val_loss did not improve from 0.07248\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.0763 - accuracy: 0.9766 - val_loss: 0.0736 - val_accuracy: 0.9768 - lr: 5.0000e-04\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9768\n",
      "Epoch 00142: val_loss improved from 0.07248 to 0.07231, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 316ms/step - loss: 0.0752 - accuracy: 0.9768 - val_loss: 0.0723 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9770\n",
      "Epoch 00143: val_loss improved from 0.07231 to 0.07219, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 315ms/step - loss: 0.0745 - accuracy: 0.9770 - val_loss: 0.0722 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9771\n",
      "Epoch 00144: val_loss did not improve from 0.07219\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.0742 - accuracy: 0.9771 - val_loss: 0.0723 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9770\n",
      "Epoch 00145: val_loss did not improve from 0.07219\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0744 - accuracy: 0.9770 - val_loss: 0.0726 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9770\n",
      "Epoch 00146: val_loss did not improve from 0.07219\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.0741 - accuracy: 0.9770 - val_loss: 0.0723 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9770\n",
      "Epoch 00147: val_loss improved from 0.07219 to 0.07213, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 315ms/step - loss: 0.0743 - accuracy: 0.9770 - val_loss: 0.0721 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9769\n",
      "Epoch 00148: val_loss improved from 0.07213 to 0.07209, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 323ms/step - loss: 0.0744 - accuracy: 0.9769 - val_loss: 0.0721 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n",
      "Epoch 00149: val_loss improved from 0.07209 to 0.07204, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 332ms/step - loss: 0.0737 - accuracy: 0.9771 - val_loss: 0.0720 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9771\n",
      "Epoch 00150: val_loss did not improve from 0.07204\n",
      "25/25 [==============================] - 8s 316ms/step - loss: 0.0740 - accuracy: 0.9771 - val_loss: 0.0721 - val_accuracy: 0.9773 - lr: 5.0000e-04\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9771\n",
      "Epoch 00151: val_loss did not improve from 0.07204\n",
      "25/25 [==============================] - 8s 315ms/step - loss: 0.0739 - accuracy: 0.9771 - val_loss: 0.0723 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9766\n",
      "Epoch 00152: val_loss did not improve from 0.07204\n",
      "25/25 [==============================] - 8s 309ms/step - loss: 0.0751 - accuracy: 0.9766 - val_loss: 0.0730 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9769\n",
      "Epoch 00153: val_loss did not improve from 0.07204\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.0744 - accuracy: 0.9769 - val_loss: 0.0725 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9767\n",
      "Epoch 00154: val_loss did not improve from 0.07204\n",
      "25/25 [==============================] - 8s 327ms/step - loss: 0.0749 - accuracy: 0.9767 - val_loss: 0.0722 - val_accuracy: 0.9773 - lr: 5.0000e-04\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9769\n",
      "Epoch 00155: val_loss improved from 0.07204 to 0.07202, saving model to best_model.h5\n",
      "25/25 [==============================] - 9s 343ms/step - loss: 0.0748 - accuracy: 0.9769 - val_loss: 0.0720 - val_accuracy: 0.9773 - lr: 5.0000e-04\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9768\n",
      "Epoch 00156: val_loss did not improve from 0.07202\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 0.0747 - accuracy: 0.9768 - val_loss: 0.0721 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9769\n",
      "Epoch 00157: val_loss improved from 0.07202 to 0.07181, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 318ms/step - loss: 0.0742 - accuracy: 0.9769 - val_loss: 0.0718 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9770\n",
      "Epoch 00158: val_loss improved from 0.07181 to 0.07177, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 0.0740 - accuracy: 0.9770 - val_loss: 0.0718 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9772\n",
      "Epoch 00159: val_loss improved from 0.07177 to 0.07170, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 0.0735 - accuracy: 0.9772 - val_loss: 0.0717 - val_accuracy: 0.9773 - lr: 5.0000e-04\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9769\n",
      "Epoch 00160: val_loss did not improve from 0.07170\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 0.0740 - accuracy: 0.9769 - val_loss: 0.0725 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9768\n",
      "Epoch 00161: val_loss did not improve from 0.07170\n",
      "25/25 [==============================] - 8s 316ms/step - loss: 0.0747 - accuracy: 0.9768 - val_loss: 0.0726 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9768\n",
      "Epoch 00162: val_loss did not improve from 0.07170\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 0.0739 - accuracy: 0.9768 - val_loss: 0.0724 - val_accuracy: 0.9770 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9770\n",
      "Epoch 00163: val_loss did not improve from 0.07170\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 0.0735 - accuracy: 0.9770 - val_loss: 0.0721 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9770\n",
      "Epoch 00164: val_loss did not improve from 0.07170\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.0727 - val_accuracy: 0.9768 - lr: 5.0000e-04\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9769\n",
      "Epoch 00165: val_loss did not improve from 0.07170\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 0.0719 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9771\n",
      "Epoch 00166: val_loss improved from 0.07170 to 0.07160, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 0.0733 - accuracy: 0.9771 - val_loss: 0.0716 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9769\n",
      "Epoch 00167: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0737 - accuracy: 0.9769 - val_loss: 0.0719 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9770\n",
      "Epoch 00168: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.0721 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n",
      "Epoch 00169: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0738 - accuracy: 0.9771 - val_loss: 0.0718 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9769\n",
      "Epoch 00170: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0740 - accuracy: 0.9769 - val_loss: 0.0754 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9765\n",
      "Epoch 00171: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0748 - accuracy: 0.9765 - val_loss: 0.0724 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9770\n",
      "Epoch 00172: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0741 - accuracy: 0.9770 - val_loss: 0.0721 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9769\n",
      "Epoch 00173: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0738 - accuracy: 0.9769 - val_loss: 0.0730 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9749\n",
      "Epoch 00174: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0804 - accuracy: 0.9749 - val_loss: 0.1113 - val_accuracy: 0.9659 - lr: 5.0000e-04\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9717\n",
      "Epoch 00175: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0941 - accuracy: 0.9717 - val_loss: 0.0773 - val_accuracy: 0.9762 - lr: 5.0000e-04\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9763\n",
      "Epoch 00176: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0787 - accuracy: 0.9763 - val_loss: 0.0734 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9765\n",
      "Epoch 00177: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0763 - accuracy: 0.9765 - val_loss: 0.0730 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9768\n",
      "Epoch 00178: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0749 - accuracy: 0.9768 - val_loss: 0.0733 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9768\n",
      "Epoch 00179: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0755 - accuracy: 0.9768 - val_loss: 0.0732 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9770\n",
      "Epoch 00180: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 292ms/step - loss: 0.0748 - accuracy: 0.9770 - val_loss: 0.0730 - val_accuracy: 0.9771 - lr: 5.0000e-04\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9770\n",
      "Epoch 00181: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0741 - accuracy: 0.9770 - val_loss: 0.0722 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9771\n",
      "Epoch 00182: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0742 - accuracy: 0.9771 - val_loss: 0.0725 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9766\n",
      "Epoch 00183: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0747 - accuracy: 0.9766 - val_loss: 0.0725 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9770\n",
      "Epoch 00184: val_loss did not improve from 0.07160\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0741 - accuracy: 0.9770 - val_loss: 0.0722 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9772\n",
      "Epoch 00185: val_loss improved from 0.07160 to 0.07149, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0730 - accuracy: 0.9772 - val_loss: 0.0715 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9771\n",
      "Epoch 00186: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.0730 - accuracy: 0.9771 - val_loss: 0.0716 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9769\n",
      "Epoch 00187: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 0.0720 - val_accuracy: 0.9773 - lr: 5.0000e-04\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9770\n",
      "Epoch 00188: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0737 - accuracy: 0.9770 - val_loss: 0.0720 - val_accuracy: 0.9770 - lr: 5.0000e-04\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9770\n",
      "Epoch 00189: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.0719 - val_accuracy: 0.9772 - lr: 5.0000e-04\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9675\n",
      "Epoch 00190: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 311ms/step - loss: 0.1051 - accuracy: 0.9675 - val_loss: 0.0888 - val_accuracy: 0.9738 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9663\n",
      "Epoch 00191: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 299ms/step - loss: 0.1182 - accuracy: 0.9663 - val_loss: 0.1533 - val_accuracy: 0.9509 - lr: 5.0000e-04\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6978 - accuracy: 0.8210\n",
      "Epoch 00192: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.6978 - accuracy: 0.8210 - val_loss: 0.7130 - val_accuracy: 0.8453 - lr: 5.0000e-04\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9330\n",
      "Epoch 00193: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.2599 - accuracy: 0.9330 - val_loss: 0.1650 - val_accuracy: 0.9473 - lr: 5.0000e-04\n",
      "Epoch 194/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9661\n",
      "Epoch 00194: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.1268 - accuracy: 0.9661 - val_loss: 0.0971 - val_accuracy: 0.9724 - lr: 5.0000e-04\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9714\n",
      "Epoch 00195: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.1049 - accuracy: 0.9714 - val_loss: 0.1058 - val_accuracy: 0.9731 - lr: 5.0000e-04\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9728\n",
      "Epoch 00196: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0921 - accuracy: 0.9728 - val_loss: 0.0920 - val_accuracy: 0.9725 - lr: 5.0000e-04\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9739\n",
      "Epoch 00197: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0891 - accuracy: 0.9739 - val_loss: 0.0859 - val_accuracy: 0.9745 - lr: 5.0000e-04\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9726\n",
      "Epoch 00198: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0931 - accuracy: 0.9726 - val_loss: 0.1023 - val_accuracy: 0.9711 - lr: 5.0000e-04\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9734\n",
      "Epoch 00199: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0914 - accuracy: 0.9734 - val_loss: 0.0789 - val_accuracy: 0.9764 - lr: 5.0000e-04\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9746\n",
      "Epoch 00200: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0856 - accuracy: 0.9746 - val_loss: 0.0773 - val_accuracy: 0.9767 - lr: 5.0000e-04\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9752\n",
      "Epoch 00201: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0837 - accuracy: 0.9752 - val_loss: 0.0769 - val_accuracy: 0.9767 - lr: 1.0000e-04\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9734\n",
      "Epoch 00202: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 0.0879 - accuracy: 0.9734 - val_loss: 0.0767 - val_accuracy: 0.9768 - lr: 1.0000e-04\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9753\n",
      "Epoch 00203: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 0.0831 - accuracy: 0.9753 - val_loss: 0.0773 - val_accuracy: 0.9767 - lr: 1.0000e-04\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9738\n",
      "Epoch 00204: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.0867 - accuracy: 0.9738 - val_loss: 0.0766 - val_accuracy: 0.9768 - lr: 1.0000e-04\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9754\n",
      "Epoch 00205: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0824 - accuracy: 0.9754 - val_loss: 0.0763 - val_accuracy: 0.9767 - lr: 1.0000e-04\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9760\n",
      "Epoch 00206: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 0.0803 - accuracy: 0.9760 - val_loss: 0.0761 - val_accuracy: 0.9768 - lr: 1.0000e-04\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9750\n",
      "Epoch 00207: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0830 - accuracy: 0.9750 - val_loss: 0.0760 - val_accuracy: 0.9768 - lr: 1.0000e-04\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9758\n",
      "Epoch 00208: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0819 - accuracy: 0.9758 - val_loss: 0.0764 - val_accuracy: 0.9768 - lr: 1.0000e-04\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.9753\n",
      "Epoch 00209: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0827 - accuracy: 0.9753 - val_loss: 0.0761 - val_accuracy: 0.9768 - lr: 1.0000e-04\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9756\n",
      "Epoch 00210: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0816 - accuracy: 0.9756 - val_loss: 0.0758 - val_accuracy: 0.9769 - lr: 1.0000e-04\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9758\n",
      "Epoch 00211: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0813 - accuracy: 0.9758 - val_loss: 0.0758 - val_accuracy: 0.9769 - lr: 1.0000e-04\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9758\n",
      "Epoch 00212: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0809 - accuracy: 0.9758 - val_loss: 0.0755 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9761\n",
      "Epoch 00213: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0798 - accuracy: 0.9761 - val_loss: 0.0755 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9759\n",
      "Epoch 00214: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0799 - accuracy: 0.9759 - val_loss: 0.0754 - val_accuracy: 0.9769 - lr: 1.0000e-04\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9754\n",
      "Epoch 00215: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 293ms/step - loss: 0.0822 - accuracy: 0.9754 - val_loss: 0.0757 - val_accuracy: 0.9769 - lr: 1.0000e-04\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9757\n",
      "Epoch 00216: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 0.0816 - accuracy: 0.9757 - val_loss: 0.0754 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9758\n",
      "Epoch 00217: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 324ms/step - loss: 0.0812 - accuracy: 0.9758 - val_loss: 0.0754 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9763\n",
      "Epoch 00218: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 307ms/step - loss: 0.0793 - accuracy: 0.9763 - val_loss: 0.0752 - val_accuracy: 0.9770 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9758\n",
      "Epoch 00219: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0802 - accuracy: 0.9758 - val_loss: 0.0753 - val_accuracy: 0.9769 - lr: 1.0000e-04\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9757\n",
      "Epoch 00220: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.0808 - accuracy: 0.9757 - val_loss: 0.0752 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 221/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9759\n",
      "Epoch 00221: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0804 - accuracy: 0.9759 - val_loss: 0.0753 - val_accuracy: 0.9769 - lr: 1.0000e-04\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9760\n",
      "Epoch 00222: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0798 - accuracy: 0.9760 - val_loss: 0.0749 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9760\n",
      "Epoch 00223: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 300ms/step - loss: 0.0801 - accuracy: 0.9760 - val_loss: 0.0751 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9762\n",
      "Epoch 00224: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0790 - accuracy: 0.9762 - val_loss: 0.0747 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9759\n",
      "Epoch 00225: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0795 - accuracy: 0.9759 - val_loss: 0.0748 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9763\n",
      "Epoch 00226: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0784 - accuracy: 0.9763 - val_loss: 0.0746 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9757\n",
      "Epoch 00227: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.0806 - accuracy: 0.9757 - val_loss: 0.0752 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9762\n",
      "Epoch 00228: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0790 - accuracy: 0.9762 - val_loss: 0.0746 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9763\n",
      "Epoch 00229: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 321ms/step - loss: 0.0787 - accuracy: 0.9763 - val_loss: 0.0745 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9760\n",
      "Epoch 00230: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 320ms/step - loss: 0.0803 - accuracy: 0.9760 - val_loss: 0.0745 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9765\n",
      "Epoch 00231: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 308ms/step - loss: 0.0776 - accuracy: 0.9765 - val_loss: 0.0743 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9764\n",
      "Epoch 00232: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 317ms/step - loss: 0.0779 - accuracy: 0.9764 - val_loss: 0.0743 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9764\n",
      "Epoch 00233: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 315ms/step - loss: 0.0780 - accuracy: 0.9764 - val_loss: 0.0744 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9763\n",
      "Epoch 00234: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.0787 - accuracy: 0.9763 - val_loss: 0.0743 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9762\n",
      "Epoch 00235: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0787 - accuracy: 0.9762 - val_loss: 0.0744 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9764\n",
      "Epoch 00236: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 300ms/step - loss: 0.0782 - accuracy: 0.9764 - val_loss: 0.0742 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9764\n",
      "Epoch 00237: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.0779 - accuracy: 0.9764 - val_loss: 0.0741 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9763\n",
      "Epoch 00238: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 314ms/step - loss: 0.0784 - accuracy: 0.9763 - val_loss: 0.0742 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9764\n",
      "Epoch 00239: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 312ms/step - loss: 0.0780 - accuracy: 0.9764 - val_loss: 0.0740 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9764\n",
      "Epoch 00240: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 301ms/step - loss: 0.0780 - accuracy: 0.9764 - val_loss: 0.0741 - val_accuracy: 0.9770 - lr: 1.0000e-04\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9763\n",
      "Epoch 00241: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 300ms/step - loss: 0.0780 - accuracy: 0.9763 - val_loss: 0.0740 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9766\n",
      "Epoch 00242: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0773 - accuracy: 0.9766 - val_loss: 0.0740 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9764\n",
      "Epoch 00243: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0777 - accuracy: 0.9764 - val_loss: 0.0739 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9766\n",
      "Epoch 00244: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 313ms/step - loss: 0.0768 - accuracy: 0.9766 - val_loss: 0.0737 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9765\n",
      "Epoch 00245: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0779 - accuracy: 0.9765 - val_loss: 0.0738 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9765\n",
      "Epoch 00246: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0777 - accuracy: 0.9765 - val_loss: 0.0737 - val_accuracy: 0.9771 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9767\n",
      "Epoch 00247: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0768 - accuracy: 0.9767 - val_loss: 0.0736 - val_accuracy: 0.9772 - lr: 1.0000e-04\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9761\n",
      "Epoch 00248: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.0788 - accuracy: 0.9761 - val_loss: 0.0736 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 249/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9761\n",
      "Epoch 00249: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 303ms/step - loss: 0.0788 - accuracy: 0.9761 - val_loss: 0.0738 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9765\n",
      "Epoch 00250: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0773 - accuracy: 0.9765 - val_loss: 0.0736 - val_accuracy: 0.9772 - lr: 1.0000e-04\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9765\n",
      "Epoch 00251: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 294ms/step - loss: 0.0772 - accuracy: 0.9765 - val_loss: 0.0736 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9767\n",
      "Epoch 00252: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0769 - accuracy: 0.9767 - val_loss: 0.0737 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9767\n",
      "Epoch 00253: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0767 - accuracy: 0.9767 - val_loss: 0.0734 - val_accuracy: 0.9772 - lr: 1.0000e-04\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9767\n",
      "Epoch 00254: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0765 - accuracy: 0.9767 - val_loss: 0.0736 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9767\n",
      "Epoch 00255: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.0768 - accuracy: 0.9767 - val_loss: 0.0734 - val_accuracy: 0.9772 - lr: 1.0000e-04\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9765\n",
      "Epoch 00256: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0773 - accuracy: 0.9765 - val_loss: 0.0733 - val_accuracy: 0.9772 - lr: 1.0000e-04\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9768\n",
      "Epoch 00257: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0764 - accuracy: 0.9768 - val_loss: 0.0734 - val_accuracy: 0.9771 - lr: 1.0000e-04\n",
      "Epoch 258/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9764\n",
      "Epoch 00258: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 7s 300ms/step - loss: 0.0770 - accuracy: 0.9764 - val_loss: 0.0733 - val_accuracy: 0.9772 - lr: 1.0000e-04\n",
      "Epoch 259/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9768\n",
      "Epoch 00259: val_loss did not improve from 0.07149\n",
      "25/25 [==============================] - 8s 320ms/step - loss: 0.0761 - accuracy: 0.9768 - val_loss: 0.0732 - val_accuracy: 0.9772 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "          validation_data=(X_validation,y_validation), callbacks=[mc, es, lrs]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim((0.94,0.98))\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.977, Test: 0.977\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the model from saved or use dierectly - uncomment as needed\n",
    "#saved_model = load_model('best_model.h5')\n",
    "saved_model = model\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(X_validation, y_validation, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score:  0.95693\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax((saved_model.predict(X_validation)+saved_model.predict(X_validation[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\n",
    "gt = np.argmax(y_validation, axis=2).reshape(-1)\n",
    "print(\"Macro F1 score: \", np.round(f1_score(gt, pred, average=\"macro\"), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.save('../models/model_2.2.6_11.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax((saved_model.predict(X_test)+saved_model.predict(X_test[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['open_channels']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['time','open_channels']].to_csv('../reports/ion_submission_Wavenet9_2.2.6.csv', \n",
    "                                                  index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>signal</th>\n",
       "      <th>cat_pred</th>\n",
       "      <th>exp</th>\n",
       "      <th>power2</th>\n",
       "      <th>sqroot</th>\n",
       "      <th>open_channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0001</td>\n",
       "      <td>-2.6504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>7.0312</td>\n",
       "      <td>-1.6279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500.0002</td>\n",
       "      <td>-2.8457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>8.1016</td>\n",
       "      <td>-1.6875</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500.0003</td>\n",
       "      <td>-2.8535</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>8.1406</td>\n",
       "      <td>-1.6895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500.0004</td>\n",
       "      <td>-2.4434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>5.9727</td>\n",
       "      <td>-1.5635</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500.0005</td>\n",
       "      <td>-2.6133</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0734</td>\n",
       "      <td>6.8242</td>\n",
       "      <td>-1.6162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time  signal  cat_pred     exp  power2  sqroot  open_channels\n",
       "0  500.0001 -2.6504         0  0.0706  7.0312 -1.6279              0\n",
       "1  500.0002 -2.8457         0  0.0580  8.1016 -1.6875              0\n",
       "2  500.0003 -2.8535         0  0.0576  8.1406 -1.6895              0\n",
       "3  500.0004 -2.4434         0  0.0869  5.9727 -1.5635              0\n",
       "4  500.0005 -2.6133         0  0.0734  6.8242 -1.6162              0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 86\n",
    "Wavenet 6 features 4 layers 64 filters 3 kernels - normalize after CNN 1 (no dropout) - LR 0.001 120 epochs\n",
    "\n",
    "LB 0.94 F1 0.95518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input = Input(shape=(input_shape))\n",
    "x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
    "x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
    "x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
    "x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
    "# x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
    "# x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
    "x = BatchNormalization()(x)\n",
    "l_output = Dense(11, activation='softmax')(x)\n",
    "model = models.Model(inputs=[l_input], outputs=[l_output])\n",
    "opt = Adam(lr=LR)\n",
    "opt = tfa.optimizers.SWA(opt)\n",
    "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.977, Test: 0.977\n"
     ]
    }
   ],
   "source": [
    "saved_model = load_model('best_model.h5')\n",
    "#saved_model = model\n",
    "# evaluate the model\n",
    "_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(X_validation, y_validation, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score:  0.95703\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax((saved_model.predict(X_validation)+saved_model.predict(X_validation[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\n",
    "gt = np.argmax(y_validation, axis=2).reshape(-1)\n",
    "print(\"Macro F1 score: \", np.round(f1_score(gt, pred, average=\"macro\"), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Maxpooling: model 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_ = 64 \n",
    "kernel_size_ = 3\n",
    "verbose, epochs, batch_size = 1, 500, 32\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[2]\n",
    "stacked_layers_ = [8, 4, 1]\n",
    "input_shape = (n_timesteps, n_features)\n",
    "LR = 0.0001\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=50)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "lrs = LearningRateScheduler(decay_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input = Input(shape=(input_shape))\n",
    "x = Conv1D(num_filters_, 1, padding='same')(l_input)\n",
    "x = MaxPooling1D(pool_size=2, strides=1, padding=\"same\") (x)\n",
    "#, bias_regularizer=l2(0.01), kernel_regularizer=l2(0.01)\n",
    "x = BatchNormalization()(x)\n",
    "x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n",
    "x = Conv1D(num_filters_*2, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n",
    "x = Conv1D(num_filters_*4, 1, padding='same')(x)\n",
    "x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n",
    "# x = Conv1D(num_filters_*8, 1, padding='same')(x)\n",
    "# x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dense(2840, activation='relu')(x)\n",
    "l_output = Dense(11, activation='softmax')(x)\n",
    "model = models.Model(inputs=[l_input], outputs=[l_output])\n",
    "opt = Adam(lr=LR)\n",
    "opt = tfa.optimizers.SWA(opt)\n",
    "model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 5000, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 5000, 64)     448         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5000, 64)     0           conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5000, 64)     256         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 5000, 64)     12352       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 5000, 64)     12352       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_29 (Multiply)          (None, 5000, 64)     0           conv1d_95[0][0]                  \n",
      "                                                                 conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 5000, 64)     4160        multiply_29[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, 5000, 64)     12352       conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, 5000, 64)     12352       conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_30 (Multiply)          (None, 5000, 64)     0           conv1d_98[0][0]                  \n",
      "                                                                 conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, 5000, 64)     4160        multiply_30[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, 5000, 64)     12352       conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, 5000, 64)     12352       conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_31 (Multiply)          (None, 5000, 64)     0           conv1d_101[0][0]                 \n",
      "                                                                 conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, 5000, 64)     4160        multiply_31[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, 5000, 64)     12352       conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, 5000, 64)     12352       conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_32 (Multiply)          (None, 5000, 64)     0           conv1d_104[0][0]                 \n",
      "                                                                 conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, 5000, 64)     4160        multiply_32[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, 5000, 64)     12352       conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, 5000, 64)     12352       conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_33 (Multiply)          (None, 5000, 64)     0           conv1d_107[0][0]                 \n",
      "                                                                 conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, 5000, 64)     4160        multiply_33[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, 5000, 64)     12352       conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, 5000, 64)     12352       conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_34 (Multiply)          (None, 5000, 64)     0           conv1d_110[0][0]                 \n",
      "                                                                 conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 5000, 64)     0           batch_normalization_2[0][0]      \n",
      "                                                                 conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, 5000, 64)     4160        multiply_34[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 5000, 64)     0           add_29[0][0]                     \n",
      "                                                                 conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, 5000, 64)     12352       conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, 5000, 64)     12352       conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 5000, 64)     0           add_30[0][0]                     \n",
      "                                                                 conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_35 (Multiply)          (None, 5000, 64)     0           conv1d_113[0][0]                 \n",
      "                                                                 conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 5000, 64)     0           add_31[0][0]                     \n",
      "                                                                 conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, 5000, 64)     4160        multiply_35[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 5000, 64)     0           add_32[0][0]                     \n",
      "                                                                 conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, 5000, 64)     12352       conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, 5000, 64)     12352       conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 5000, 64)     0           add_33[0][0]                     \n",
      "                                                                 conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_36 (Multiply)          (None, 5000, 64)     0           conv1d_116[0][0]                 \n",
      "                                                                 conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 5000, 64)     0           add_34[0][0]                     \n",
      "                                                                 conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, 5000, 64)     4160        multiply_36[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 5000, 64)     0           add_35[0][0]                     \n",
      "                                                                 conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, 5000, 128)    8320        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, 5000, 128)    49280       conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, 5000, 128)    49280       conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_37 (Multiply)          (None, 5000, 128)    0           conv1d_120[0][0]                 \n",
      "                                                                 conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, 5000, 128)    16512       multiply_37[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, 5000, 128)    49280       conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, 5000, 128)    49280       conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_38 (Multiply)          (None, 5000, 128)    0           conv1d_123[0][0]                 \n",
      "                                                                 conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, 5000, 128)    16512       multiply_38[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, 5000, 128)    49280       conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, 5000, 128)    49280       conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_39 (Multiply)          (None, 5000, 128)    0           conv1d_126[0][0]                 \n",
      "                                                                 conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, 5000, 128)    16512       multiply_39[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 5000, 128)    0           conv1d_119[0][0]                 \n",
      "                                                                 conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, 5000, 128)    49280       conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, 5000, 128)    49280       conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 5000, 128)    0           add_37[0][0]                     \n",
      "                                                                 conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_40 (Multiply)          (None, 5000, 128)    0           conv1d_129[0][0]                 \n",
      "                                                                 conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 5000, 128)    0           add_38[0][0]                     \n",
      "                                                                 conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, 5000, 128)    16512       multiply_40[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 5000, 128)    0           add_39[0][0]                     \n",
      "                                                                 conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, 5000, 256)    33024       add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, 5000, 256)    196864      conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, 5000, 256)    196864      conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_41 (Multiply)          (None, 5000, 256)    0           conv1d_133[0][0]                 \n",
      "                                                                 conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, 5000, 256)    65792       multiply_41[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 5000, 256)    0           conv1d_132[0][0]                 \n",
      "                                                                 conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5000, 11)     2827        add_41[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,195,595\n",
      "Trainable params: 1,195,467\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.5044 - accuracy: 0.4442\n",
      "Epoch 00001: val_loss improved from inf to 7.49040, saving model to best_model.h5\n",
      "25/25 [==============================] - 9s 347ms/step - loss: 1.5044 - accuracy: 0.4442 - val_loss: 7.4904 - val_accuracy: 0.0665 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.6167\n",
      "Epoch 00002: val_loss improved from 7.49040 to 4.18137, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 319ms/step - loss: 1.0039 - accuracy: 0.6167 - val_loss: 4.1814 - val_accuracy: 0.3144 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.8107 - accuracy: 0.6922\n",
      "Epoch 00003: val_loss improved from 4.18137 to 1.72842, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 322ms/step - loss: 0.8107 - accuracy: 0.6922 - val_loss: 1.7284 - val_accuracy: 0.4082 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.7884\n",
      "Epoch 00004: val_loss improved from 1.72842 to 1.72391, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 310ms/step - loss: 0.6084 - accuracy: 0.7884 - val_loss: 1.7239 - val_accuracy: 0.4215 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.8276\n",
      "Epoch 00005: val_loss improved from 1.72391 to 1.06783, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 302ms/step - loss: 0.4900 - accuracy: 0.8276 - val_loss: 1.0678 - val_accuracy: 0.6267 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3800 - accuracy: 0.8636\n",
      "Epoch 00006: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.3800 - accuracy: 0.8636 - val_loss: 1.3612 - val_accuracy: 0.5734 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.8720\n",
      "Epoch 00007: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.3618 - accuracy: 0.8720 - val_loss: 1.1205 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8826\n",
      "Epoch 00008: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.3198 - accuracy: 0.8826 - val_loss: 1.3877 - val_accuracy: 0.5311 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.8949\n",
      "Epoch 00009: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 295ms/step - loss: 0.2932 - accuracy: 0.8949 - val_loss: 1.4855 - val_accuracy: 0.6445 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.9045\n",
      "Epoch 00010: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.2581 - accuracy: 0.9045 - val_loss: 1.3353 - val_accuracy: 0.6733 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9066\n",
      "Epoch 00011: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.2491 - accuracy: 0.9066 - val_loss: 1.1762 - val_accuracy: 0.6695 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.9060\n",
      "Epoch 00012: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.2500 - accuracy: 0.9060 - val_loss: 1.2231 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9044\n",
      "Epoch 00013: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.2567 - accuracy: 0.9044 - val_loss: 1.2882 - val_accuracy: 0.6688 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9199\n",
      "Epoch 00014: val_loss did not improve from 1.06783\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.2151 - accuracy: 0.9199 - val_loss: 1.1818 - val_accuracy: 0.6454 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9294\n",
      "Epoch 00015: val_loss improved from 1.06783 to 0.66491, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1874 - accuracy: 0.9294 - val_loss: 0.6649 - val_accuracy: 0.7594 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9259\n",
      "Epoch 00016: val_loss improved from 0.66491 to 0.58062, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1962 - accuracy: 0.9259 - val_loss: 0.5806 - val_accuracy: 0.7900 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9373\n",
      "Epoch 00017: val_loss did not improve from 0.58062\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.1661 - accuracy: 0.9373 - val_loss: 0.6537 - val_accuracy: 0.7637 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9475\n",
      "Epoch 00018: val_loss did not improve from 0.58062\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.1438 - accuracy: 0.9475 - val_loss: 0.5901 - val_accuracy: 0.7854 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9422\n",
      "Epoch 00019: val_loss improved from 0.58062 to 0.37629, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1565 - accuracy: 0.9422 - val_loss: 0.3763 - val_accuracy: 0.8566 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1434 - accuracy: 0.9475\n",
      "Epoch 00020: val_loss did not improve from 0.37629\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.1434 - accuracy: 0.9475 - val_loss: 0.4667 - val_accuracy: 0.7967 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9389\n",
      "Epoch 00021: val_loss improved from 0.37629 to 0.35428, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1667 - accuracy: 0.9389 - val_loss: 0.3543 - val_accuracy: 0.8737 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9504\n",
      "Epoch 00022: val_loss improved from 0.35428 to 0.33948, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1373 - accuracy: 0.9504 - val_loss: 0.3395 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9453\n",
      "Epoch 00023: val_loss improved from 0.33948 to 0.16568, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1515 - accuracy: 0.9453 - val_loss: 0.1657 - val_accuracy: 0.9419 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9508\n",
      "Epoch 00024: val_loss did not improve from 0.16568\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.1387 - accuracy: 0.9508 - val_loss: 0.1838 - val_accuracy: 0.9305 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9548\n",
      "Epoch 00025: val_loss improved from 0.16568 to 0.12368, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.1280 - accuracy: 0.9548 - val_loss: 0.1237 - val_accuracy: 0.9609 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9528\n",
      "Epoch 00026: val_loss improved from 0.12368 to 0.10651, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1336 - accuracy: 0.9528 - val_loss: 0.1065 - val_accuracy: 0.9641 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9118\n",
      "Epoch 00027: val_loss did not improve from 0.10651\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.2646 - accuracy: 0.9118 - val_loss: 0.2679 - val_accuracy: 0.8902 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9366\n",
      "Epoch 00028: val_loss did not improve from 0.10651\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1833 - accuracy: 0.9366 - val_loss: 0.1519 - val_accuracy: 0.9503 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9504\n",
      "Epoch 00029: val_loss did not improve from 0.10651\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1424 - accuracy: 0.9504 - val_loss: 0.1323 - val_accuracy: 0.9530 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9546\n",
      "Epoch 00030: val_loss did not improve from 0.10651\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1296 - accuracy: 0.9546 - val_loss: 0.1349 - val_accuracy: 0.9530 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9561\n",
      "Epoch 00031: val_loss did not improve from 0.10651\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1259 - accuracy: 0.9561 - val_loss: 0.1245 - val_accuracy: 0.9556 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9597\n",
      "Epoch 00032: val_loss did not improve from 0.10651\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1163 - accuracy: 0.9597 - val_loss: 0.1097 - val_accuracy: 0.9616 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9602\n",
      "Epoch 00033: val_loss improved from 0.10651 to 0.09574, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1156 - accuracy: 0.9602 - val_loss: 0.0957 - val_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9404\n",
      "Epoch 00034: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1763 - accuracy: 0.9404 - val_loss: 0.1647 - val_accuracy: 0.9432 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9418\n",
      "Epoch 00035: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1650 - accuracy: 0.9418 - val_loss: 0.1470 - val_accuracy: 0.9480 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9501\n",
      "Epoch 00036: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1427 - accuracy: 0.9501 - val_loss: 0.1338 - val_accuracy: 0.9552 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.8748\n",
      "Epoch 00037: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.4622 - accuracy: 0.8748 - val_loss: 0.3935 - val_accuracy: 0.8515 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9029\n",
      "Epoch 00038: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.2913 - accuracy: 0.9029 - val_loss: 0.5034 - val_accuracy: 0.7861 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9467\n",
      "Epoch 00039: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1540 - accuracy: 0.9467 - val_loss: 0.1442 - val_accuracy: 0.9504 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9464\n",
      "Epoch 00040: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1513 - accuracy: 0.9464 - val_loss: 0.1200 - val_accuracy: 0.9600 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1370 - accuracy: 0.9516\n",
      "Epoch 00041: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1370 - accuracy: 0.9516 - val_loss: 0.1105 - val_accuracy: 0.9622 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9508\n",
      "Epoch 00042: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1381 - accuracy: 0.9508 - val_loss: 0.1432 - val_accuracy: 0.9504 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9589\n",
      "Epoch 00043: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1196 - accuracy: 0.9589 - val_loss: 0.0993 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9560\n",
      "Epoch 00044: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1261 - accuracy: 0.9560 - val_loss: 0.1008 - val_accuracy: 0.9655 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9552\n",
      "Epoch 00045: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1279 - accuracy: 0.9552 - val_loss: 0.1532 - val_accuracy: 0.9429 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9537\n",
      "Epoch 00046: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1348 - accuracy: 0.9537 - val_loss: 0.1152 - val_accuracy: 0.9604 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1229 - accuracy: 0.9593\n",
      "Epoch 00047: val_loss did not improve from 0.09574\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1229 - accuracy: 0.9593 - val_loss: 0.1176 - val_accuracy: 0.9599 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9630\n",
      "Epoch 00048: val_loss improved from 0.09574 to 0.09561, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1096 - accuracy: 0.9630 - val_loss: 0.0956 - val_accuracy: 0.9677 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9590\n",
      "Epoch 00049: val_loss did not improve from 0.09561\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1211 - accuracy: 0.9590 - val_loss: 0.1101 - val_accuracy: 0.9618 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1188 - accuracy: 0.9607\n",
      "Epoch 00050: val_loss did not improve from 0.09561\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1188 - accuracy: 0.9607 - val_loss: 0.1025 - val_accuracy: 0.9675 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9649\n",
      "Epoch 00051: val_loss did not improve from 0.09561\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1061 - accuracy: 0.9649 - val_loss: 0.1001 - val_accuracy: 0.9661 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9664\n",
      "Epoch 00052: val_loss improved from 0.09561 to 0.09367, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1010 - accuracy: 0.9664 - val_loss: 0.0937 - val_accuracy: 0.9688 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9681\n",
      "Epoch 00053: val_loss improved from 0.09367 to 0.09137, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0966 - accuracy: 0.9681 - val_loss: 0.0914 - val_accuracy: 0.9698 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1009 - accuracy: 0.9664\n",
      "Epoch 00054: val_loss did not improve from 0.09137\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1009 - accuracy: 0.9664 - val_loss: 0.0950 - val_accuracy: 0.9685 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9669\n",
      "Epoch 00055: val_loss did not improve from 0.09137\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1005 - accuracy: 0.9669 - val_loss: 0.0967 - val_accuracy: 0.9676 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9653\n",
      "Epoch 00056: val_loss improved from 0.09137 to 0.08884, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1049 - accuracy: 0.9653 - val_loss: 0.0888 - val_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9677\n",
      "Epoch 00057: val_loss improved from 0.08884 to 0.08802, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0985 - accuracy: 0.9677 - val_loss: 0.0880 - val_accuracy: 0.9713 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9666\n",
      "Epoch 00058: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1015 - accuracy: 0.9666 - val_loss: 0.0929 - val_accuracy: 0.9694 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9659\n",
      "Epoch 00059: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1027 - accuracy: 0.9659 - val_loss: 0.0902 - val_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 60/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9673\n",
      "Epoch 00060: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1001 - accuracy: 0.9673 - val_loss: 0.1005 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 61/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9650\n",
      "Epoch 00061: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1045 - accuracy: 0.9650 - val_loss: 0.1151 - val_accuracy: 0.9615 - lr: 0.0010\n",
      "Epoch 62/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9605\n",
      "Epoch 00062: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.1162 - accuracy: 0.9605 - val_loss: 0.1555 - val_accuracy: 0.9503 - lr: 0.0010\n",
      "Epoch 63/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9684\n",
      "Epoch 00063: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0966 - accuracy: 0.9684 - val_loss: 0.1328 - val_accuracy: 0.9571 - lr: 0.0010\n",
      "Epoch 64/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9675\n",
      "Epoch 00064: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0989 - accuracy: 0.9675 - val_loss: 0.0910 - val_accuracy: 0.9710 - lr: 0.0010\n",
      "Epoch 65/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9683\n",
      "Epoch 00065: val_loss did not improve from 0.08802\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0967 - accuracy: 0.9683 - val_loss: 0.0950 - val_accuracy: 0.9684 - lr: 0.0010\n",
      "Epoch 66/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9646\n",
      "Epoch 00066: val_loss improved from 0.08802 to 0.08724, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.1080 - accuracy: 0.9646 - val_loss: 0.0872 - val_accuracy: 0.9718 - lr: 0.0010\n",
      "Epoch 67/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9688\n",
      "Epoch 00067: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0955 - accuracy: 0.9688 - val_loss: 0.0902 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 68/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9693\n",
      "Epoch 00068: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0940 - accuracy: 0.9693 - val_loss: 0.0933 - val_accuracy: 0.9695 - lr: 0.0010\n",
      "Epoch 69/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9690\n",
      "Epoch 00069: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0957 - accuracy: 0.9690 - val_loss: 0.0907 - val_accuracy: 0.9712 - lr: 0.0010\n",
      "Epoch 70/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9693\n",
      "Epoch 00070: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0946 - accuracy: 0.9693 - val_loss: 0.1287 - val_accuracy: 0.9551 - lr: 0.0010\n",
      "Epoch 71/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9678\n",
      "Epoch 00071: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0990 - accuracy: 0.9678 - val_loss: 0.0922 - val_accuracy: 0.9698 - lr: 0.0010\n",
      "Epoch 72/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9676\n",
      "Epoch 00072: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0989 - accuracy: 0.9676 - val_loss: 0.0906 - val_accuracy: 0.9707 - lr: 0.0010\n",
      "Epoch 73/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9691\n",
      "Epoch 00073: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0950 - accuracy: 0.9691 - val_loss: 0.0917 - val_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 74/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9696\n",
      "Epoch 00074: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0943 - accuracy: 0.9696 - val_loss: 0.0998 - val_accuracy: 0.9666 - lr: 0.0010\n",
      "Epoch 75/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9697\n",
      "Epoch 00075: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0932 - accuracy: 0.9697 - val_loss: 0.0933 - val_accuracy: 0.9695 - lr: 0.0010\n",
      "Epoch 76/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9706\n",
      "Epoch 00076: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0902 - accuracy: 0.9706 - val_loss: 0.0893 - val_accuracy: 0.9709 - lr: 0.0010\n",
      "Epoch 77/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9714\n",
      "Epoch 00077: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0889 - accuracy: 0.9714 - val_loss: 0.0876 - val_accuracy: 0.9716 - lr: 0.0010\n",
      "Epoch 78/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9714\n",
      "Epoch 00078: val_loss did not improve from 0.08724\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0892 - accuracy: 0.9714 - val_loss: 0.0916 - val_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 79/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9710\n",
      "Epoch 00079: val_loss improved from 0.08724 to 0.08490, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0905 - accuracy: 0.9710 - val_loss: 0.0849 - val_accuracy: 0.9726 - lr: 0.0010\n",
      "Epoch 80/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9709\n",
      "Epoch 00080: val_loss did not improve from 0.08490\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0911 - accuracy: 0.9709 - val_loss: 0.0906 - val_accuracy: 0.9710 - lr: 0.0010\n",
      "Epoch 81/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9704\n",
      "Epoch 00081: val_loss did not improve from 0.08490\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0919 - accuracy: 0.9704 - val_loss: 0.0891 - val_accuracy: 0.9711 - lr: 0.0010\n",
      "Epoch 82/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9704\n",
      "Epoch 00082: val_loss improved from 0.08490 to 0.08420, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0915 - accuracy: 0.9704 - val_loss: 0.0842 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 83/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9712\n",
      "Epoch 00083: val_loss did not improve from 0.08420\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0893 - accuracy: 0.9712 - val_loss: 0.0942 - val_accuracy: 0.9699 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9687\n",
      "Epoch 00084: val_loss did not improve from 0.08420\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0973 - accuracy: 0.9687 - val_loss: 0.0858 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 85/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9693\n",
      "Epoch 00085: val_loss did not improve from 0.08420\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0952 - accuracy: 0.9693 - val_loss: 0.0881 - val_accuracy: 0.9719 - lr: 0.0010\n",
      "Epoch 86/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9713\n",
      "Epoch 00086: val_loss improved from 0.08420 to 0.08388, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0902 - accuracy: 0.9713 - val_loss: 0.0839 - val_accuracy: 0.9734 - lr: 0.0010\n",
      "Epoch 87/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9726\n",
      "Epoch 00087: val_loss improved from 0.08388 to 0.08195, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0865 - accuracy: 0.9726 - val_loss: 0.0820 - val_accuracy: 0.9740 - lr: 0.0010\n",
      "Epoch 88/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9727\n",
      "Epoch 00088: val_loss did not improve from 0.08195\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0855 - accuracy: 0.9727 - val_loss: 0.0834 - val_accuracy: 0.9734 - lr: 0.0010\n",
      "Epoch 89/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9720\n",
      "Epoch 00089: val_loss did not improve from 0.08195\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0869 - accuracy: 0.9720 - val_loss: 0.0855 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 90/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9725\n",
      "Epoch 00090: val_loss improved from 0.08195 to 0.08162, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0863 - accuracy: 0.9725 - val_loss: 0.0816 - val_accuracy: 0.9739 - lr: 0.0010\n",
      "Epoch 91/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9723\n",
      "Epoch 00091: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0872 - accuracy: 0.9723 - val_loss: 0.0827 - val_accuracy: 0.9740 - lr: 0.0010\n",
      "Epoch 92/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9724\n",
      "Epoch 00092: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0868 - accuracy: 0.9724 - val_loss: 0.0902 - val_accuracy: 0.9709 - lr: 0.0010\n",
      "Epoch 93/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9723\n",
      "Epoch 00093: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0870 - accuracy: 0.9723 - val_loss: 0.0824 - val_accuracy: 0.9737 - lr: 0.0010\n",
      "Epoch 94/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9722\n",
      "Epoch 00094: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0870 - accuracy: 0.9722 - val_loss: 0.0882 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 95/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9720\n",
      "Epoch 00095: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0883 - accuracy: 0.9720 - val_loss: 0.0860 - val_accuracy: 0.9730 - lr: 0.0010\n",
      "Epoch 96/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9730\n",
      "Epoch 00096: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0850 - accuracy: 0.9730 - val_loss: 0.0857 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 97/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9733\n",
      "Epoch 00097: val_loss did not improve from 0.08162\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0841 - accuracy: 0.9733 - val_loss: 0.0836 - val_accuracy: 0.9733 - lr: 0.0010\n",
      "Epoch 98/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.9730\n",
      "Epoch 00098: val_loss improved from 0.08162 to 0.08085, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0852 - accuracy: 0.9730 - val_loss: 0.0808 - val_accuracy: 0.9745 - lr: 0.0010\n",
      "Epoch 99/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9732\n",
      "Epoch 00099: val_loss did not improve from 0.08085\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0846 - accuracy: 0.9732 - val_loss: 0.0861 - val_accuracy: 0.9721 - lr: 0.0010\n",
      "Epoch 100/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9724\n",
      "Epoch 00100: val_loss did not improve from 0.08085\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0857 - accuracy: 0.9724 - val_loss: 0.0852 - val_accuracy: 0.9724 - lr: 0.0010\n",
      "Epoch 101/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9729\n",
      "Epoch 00101: val_loss did not improve from 0.08085\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0848 - accuracy: 0.9729 - val_loss: 0.0810 - val_accuracy: 0.9741 - lr: 5.0000e-04\n",
      "Epoch 102/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9740\n",
      "Epoch 00102: val_loss improved from 0.08085 to 0.07941, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0820 - accuracy: 0.9740 - val_loss: 0.0794 - val_accuracy: 0.9748 - lr: 5.0000e-04\n",
      "Epoch 103/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9739\n",
      "Epoch 00103: val_loss did not improve from 0.07941\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0821 - accuracy: 0.9739 - val_loss: 0.0798 - val_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 104/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9738\n",
      "Epoch 00104: val_loss improved from 0.07941 to 0.07887, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0825 - accuracy: 0.9738 - val_loss: 0.0789 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 105/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9745\n",
      "Epoch 00105: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0810 - accuracy: 0.9745 - val_loss: 0.0800 - val_accuracy: 0.9748 - lr: 5.0000e-04\n",
      "Epoch 106/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1272 - accuracy: 0.9602\n",
      "Epoch 00106: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1272 - accuracy: 0.9602 - val_loss: 0.1358 - val_accuracy: 0.9575 - lr: 5.0000e-04\n",
      "Epoch 107/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9701\n",
      "Epoch 00107: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0946 - accuracy: 0.9701 - val_loss: 0.1016 - val_accuracy: 0.9671 - lr: 5.0000e-04\n",
      "Epoch 108/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9690\n",
      "Epoch 00108: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0963 - accuracy: 0.9690 - val_loss: 0.0928 - val_accuracy: 0.9700 - lr: 5.0000e-04\n",
      "Epoch 109/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9713\n",
      "Epoch 00109: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0912 - accuracy: 0.9713 - val_loss: 0.0885 - val_accuracy: 0.9717 - lr: 5.0000e-04\n",
      "Epoch 110/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9728\n",
      "Epoch 00110: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0875 - accuracy: 0.9728 - val_loss: 0.0826 - val_accuracy: 0.9742 - lr: 5.0000e-04\n",
      "Epoch 111/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9697\n",
      "Epoch 00111: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0949 - accuracy: 0.9697 - val_loss: 0.3542 - val_accuracy: 0.8782 - lr: 5.0000e-04\n",
      "Epoch 112/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9733\n",
      "Epoch 00112: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0853 - accuracy: 0.9733 - val_loss: 0.0940 - val_accuracy: 0.9715 - lr: 5.0000e-04\n",
      "Epoch 113/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9738\n",
      "Epoch 00113: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0838 - accuracy: 0.9738 - val_loss: 0.0815 - val_accuracy: 0.9748 - lr: 5.0000e-04\n",
      "Epoch 114/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9738\n",
      "Epoch 00114: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0841 - accuracy: 0.9738 - val_loss: 0.0824 - val_accuracy: 0.9743 - lr: 5.0000e-04\n",
      "Epoch 115/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9740\n",
      "Epoch 00115: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0830 - accuracy: 0.9740 - val_loss: 0.0840 - val_accuracy: 0.9739 - lr: 5.0000e-04\n",
      "Epoch 116/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9712\n",
      "Epoch 00116: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0912 - accuracy: 0.9712 - val_loss: 0.0904 - val_accuracy: 0.9721 - lr: 5.0000e-04\n",
      "Epoch 117/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9737\n",
      "Epoch 00117: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0851 - accuracy: 0.9737 - val_loss: 0.0830 - val_accuracy: 0.9743 - lr: 5.0000e-04\n",
      "Epoch 118/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9742\n",
      "Epoch 00118: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0830 - accuracy: 0.9742 - val_loss: 0.0810 - val_accuracy: 0.9750 - lr: 5.0000e-04\n",
      "Epoch 119/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9740\n",
      "Epoch 00119: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 298ms/step - loss: 0.0836 - accuracy: 0.9740 - val_loss: 0.0837 - val_accuracy: 0.9739 - lr: 5.0000e-04\n",
      "Epoch 120/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9745\n",
      "Epoch 00120: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0817 - accuracy: 0.9745 - val_loss: 0.0814 - val_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 121/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9745\n",
      "Epoch 00121: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0818 - accuracy: 0.9745 - val_loss: 0.0821 - val_accuracy: 0.9741 - lr: 5.0000e-04\n",
      "Epoch 122/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9746\n",
      "Epoch 00122: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0811 - accuracy: 0.9746 - val_loss: 0.0820 - val_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 123/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9746\n",
      "Epoch 00123: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0808 - accuracy: 0.9746 - val_loss: 0.0794 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 124/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9744\n",
      "Epoch 00124: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0813 - accuracy: 0.9744 - val_loss: 0.0850 - val_accuracy: 0.9734 - lr: 5.0000e-04\n",
      "Epoch 125/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9747\n",
      "Epoch 00125: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0808 - accuracy: 0.9747 - val_loss: 0.0801 - val_accuracy: 0.9749 - lr: 5.0000e-04\n",
      "Epoch 126/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9736\n",
      "Epoch 00126: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0839 - accuracy: 0.9736 - val_loss: 0.0818 - val_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 127/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9737\n",
      "Epoch 00127: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0834 - accuracy: 0.9737 - val_loss: 0.0878 - val_accuracy: 0.9728 - lr: 5.0000e-04\n",
      "Epoch 128/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9721\n",
      "Epoch 00128: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0879 - accuracy: 0.9721 - val_loss: 0.0871 - val_accuracy: 0.9729 - lr: 5.0000e-04\n",
      "Epoch 129/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9677\n",
      "Epoch 00129: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.1013 - accuracy: 0.9677 - val_loss: 0.0955 - val_accuracy: 0.9687 - lr: 5.0000e-04\n",
      "Epoch 130/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9727\n",
      "Epoch 00130: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0873 - accuracy: 0.9727 - val_loss: 0.0892 - val_accuracy: 0.9720 - lr: 5.0000e-04\n",
      "Epoch 131/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9738\n",
      "Epoch 00131: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0834 - accuracy: 0.9738 - val_loss: 0.0806 - val_accuracy: 0.9747 - lr: 5.0000e-04\n",
      "Epoch 132/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9746\n",
      "Epoch 00132: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0811 - accuracy: 0.9746 - val_loss: 0.0806 - val_accuracy: 0.9747 - lr: 5.0000e-04\n",
      "Epoch 133/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9741\n",
      "Epoch 00133: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0819 - accuracy: 0.9741 - val_loss: 0.0797 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 134/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9748\n",
      "Epoch 00134: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0801 - accuracy: 0.9748 - val_loss: 0.0799 - val_accuracy: 0.9750 - lr: 5.0000e-04\n",
      "Epoch 135/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9748\n",
      "Epoch 00135: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0804 - accuracy: 0.9748 - val_loss: 0.0800 - val_accuracy: 0.9750 - lr: 5.0000e-04\n",
      "Epoch 136/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9739\n",
      "Epoch 00136: val_loss did not improve from 0.07887\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0825 - accuracy: 0.9739 - val_loss: 0.0797 - val_accuracy: 0.9753 - lr: 5.0000e-04\n",
      "Epoch 137/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9749\n",
      "Epoch 00137: val_loss improved from 0.07887 to 0.07871, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 306ms/step - loss: 0.0800 - accuracy: 0.9749 - val_loss: 0.0787 - val_accuracy: 0.9754 - lr: 5.0000e-04\n",
      "Epoch 138/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9746\n",
      "Epoch 00138: val_loss did not improve from 0.07871\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0813 - accuracy: 0.9746 - val_loss: 0.0796 - val_accuracy: 0.9754 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9750\n",
      "Epoch 00139: val_loss improved from 0.07871 to 0.07836, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0797 - accuracy: 0.9750 - val_loss: 0.0784 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 140/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9751\n",
      "Epoch 00140: val_loss did not improve from 0.07836\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0792 - accuracy: 0.9751 - val_loss: 0.0785 - val_accuracy: 0.9756 - lr: 5.0000e-04\n",
      "Epoch 141/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9749\n",
      "Epoch 00141: val_loss did not improve from 0.07836\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0796 - accuracy: 0.9749 - val_loss: 0.0815 - val_accuracy: 0.9748 - lr: 5.0000e-04\n",
      "Epoch 142/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9746\n",
      "Epoch 00142: val_loss did not improve from 0.07836\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0806 - accuracy: 0.9746 - val_loss: 0.0807 - val_accuracy: 0.9747 - lr: 5.0000e-04\n",
      "Epoch 143/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9751\n",
      "Epoch 00143: val_loss did not improve from 0.07836\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0786 - accuracy: 0.9751 - val_loss: 0.0785 - val_accuracy: 0.9753 - lr: 5.0000e-04\n",
      "Epoch 144/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9753\n",
      "Epoch 00144: val_loss did not improve from 0.07836\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0783 - accuracy: 0.9753 - val_loss: 0.0796 - val_accuracy: 0.9754 - lr: 5.0000e-04\n",
      "Epoch 145/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9752\n",
      "Epoch 00145: val_loss improved from 0.07836 to 0.07811, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0785 - accuracy: 0.9752 - val_loss: 0.0781 - val_accuracy: 0.9757 - lr: 5.0000e-04\n",
      "Epoch 146/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9750\n",
      "Epoch 00146: val_loss did not improve from 0.07811\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0793 - accuracy: 0.9750 - val_loss: 0.0789 - val_accuracy: 0.9753 - lr: 5.0000e-04\n",
      "Epoch 147/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9751\n",
      "Epoch 00147: val_loss did not improve from 0.07811\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0790 - accuracy: 0.9751 - val_loss: 0.0784 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 148/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9743\n",
      "Epoch 00148: val_loss did not improve from 0.07811\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0808 - accuracy: 0.9743 - val_loss: 0.0791 - val_accuracy: 0.9754 - lr: 5.0000e-04\n",
      "Epoch 149/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9754\n",
      "Epoch 00149: val_loss did not improve from 0.07811\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0780 - accuracy: 0.9754 - val_loss: 0.0797 - val_accuracy: 0.9750 - lr: 5.0000e-04\n",
      "Epoch 150/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9750\n",
      "Epoch 00150: val_loss did not improve from 0.07811\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0788 - accuracy: 0.9750 - val_loss: 0.0782 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 151/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9755\n",
      "Epoch 00151: val_loss improved from 0.07811 to 0.07751, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0777 - accuracy: 0.9755 - val_loss: 0.0775 - val_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 152/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9753\n",
      "Epoch 00152: val_loss did not improve from 0.07751\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0783 - accuracy: 0.9753 - val_loss: 0.0796 - val_accuracy: 0.9753 - lr: 5.0000e-04\n",
      "Epoch 153/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9756\n",
      "Epoch 00153: val_loss improved from 0.07751 to 0.07750, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0775 - accuracy: 0.9756 - val_loss: 0.0775 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 154/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9750\n",
      "Epoch 00154: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0786 - accuracy: 0.9750 - val_loss: 0.0792 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 155/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9660\n",
      "Epoch 00155: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1089 - accuracy: 0.9660 - val_loss: 0.1022 - val_accuracy: 0.9677 - lr: 5.0000e-04\n",
      "Epoch 156/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9711\n",
      "Epoch 00156: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0911 - accuracy: 0.9711 - val_loss: 0.0934 - val_accuracy: 0.9725 - lr: 5.0000e-04\n",
      "Epoch 157/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9729\n",
      "Epoch 00157: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0870 - accuracy: 0.9729 - val_loss: 0.0858 - val_accuracy: 0.9737 - lr: 5.0000e-04\n",
      "Epoch 158/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9744\n",
      "Epoch 00158: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0821 - accuracy: 0.9744 - val_loss: 0.0803 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 159/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9749\n",
      "Epoch 00159: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0803 - accuracy: 0.9749 - val_loss: 0.0789 - val_accuracy: 0.9756 - lr: 5.0000e-04\n",
      "Epoch 160/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9742\n",
      "Epoch 00160: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0826 - accuracy: 0.9742 - val_loss: 0.0799 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 161/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9750\n",
      "Epoch 00161: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0799 - accuracy: 0.9750 - val_loss: 0.0805 - val_accuracy: 0.9749 - lr: 5.0000e-04\n",
      "Epoch 162/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9748\n",
      "Epoch 00162: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0799 - accuracy: 0.9748 - val_loss: 0.0824 - val_accuracy: 0.9746 - lr: 5.0000e-04\n",
      "Epoch 163/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9751\n",
      "Epoch 00163: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0794 - accuracy: 0.9751 - val_loss: 0.0782 - val_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 164/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9754\n",
      "Epoch 00164: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0782 - accuracy: 0.9754 - val_loss: 0.0783 - val_accuracy: 0.9756 - lr: 5.0000e-04\n",
      "Epoch 165/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9751\n",
      "Epoch 00165: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0790 - accuracy: 0.9751 - val_loss: 0.0791 - val_accuracy: 0.9756 - lr: 5.0000e-04\n",
      "Epoch 166/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9752\n",
      "Epoch 00166: val_loss did not improve from 0.07750\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0788 - accuracy: 0.9752 - val_loss: 0.0818 - val_accuracy: 0.9749 - lr: 5.0000e-04\n",
      "Epoch 167/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9755\n",
      "Epoch 00167: val_loss improved from 0.07750 to 0.07723, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0773 - accuracy: 0.9755 - val_loss: 0.0772 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 168/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9758\n",
      "Epoch 00168: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0768 - accuracy: 0.9758 - val_loss: 0.0788 - val_accuracy: 0.9757 - lr: 5.0000e-04\n",
      "Epoch 169/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9757\n",
      "Epoch 00169: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0770 - accuracy: 0.9757 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 170/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9757\n",
      "Epoch 00170: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0764 - accuracy: 0.9757 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 171/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9753\n",
      "Epoch 00171: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0772 - accuracy: 0.9753 - val_loss: 0.0782 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 172/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9751\n",
      "Epoch 00172: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0784 - accuracy: 0.9751 - val_loss: 0.0804 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 173/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9753\n",
      "Epoch 00173: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0775 - accuracy: 0.9753 - val_loss: 0.0778 - val_accuracy: 0.9757 - lr: 5.0000e-04\n",
      "Epoch 174/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9761\n",
      "Epoch 00174: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0756 - accuracy: 0.9761 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 175/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9759\n",
      "Epoch 00175: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0764 - accuracy: 0.9759 - val_loss: 0.0790 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 176/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9759\n",
      "Epoch 00176: val_loss did not improve from 0.07723\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0761 - accuracy: 0.9759 - val_loss: 0.0780 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 177/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9757\n",
      "Epoch 00177: val_loss improved from 0.07723 to 0.07702, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0765 - accuracy: 0.9757 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 5.0000e-04\n",
      "Epoch 178/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9761\n",
      "Epoch 00178: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0753 - accuracy: 0.9761 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 179/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9754\n",
      "Epoch 00179: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0771 - accuracy: 0.9754 - val_loss: 0.0779 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 180/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9759\n",
      "Epoch 00180: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0755 - accuracy: 0.9759 - val_loss: 0.0785 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 181/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9756\n",
      "Epoch 00181: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0766 - accuracy: 0.9756 - val_loss: 0.0795 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 182/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9750\n",
      "Epoch 00182: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0794 - accuracy: 0.9750 - val_loss: 0.0801 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 183/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9756\n",
      "Epoch 00183: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0770 - accuracy: 0.9756 - val_loss: 0.0809 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 184/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9758\n",
      "Epoch 00184: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0762 - accuracy: 0.9758 - val_loss: 0.0776 - val_accuracy: 0.9758 - lr: 5.0000e-04\n",
      "Epoch 185/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9756\n",
      "Epoch 00185: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0763 - accuracy: 0.9756 - val_loss: 0.0783 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 186/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9756\n",
      "Epoch 00186: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0760 - accuracy: 0.9756 - val_loss: 0.0796 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 187/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9622\n",
      "Epoch 00187: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.1138 - accuracy: 0.9622 - val_loss: 1.6227 - val_accuracy: 0.6307 - lr: 5.0000e-04\n",
      "Epoch 188/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9688\n",
      "Epoch 00188: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0984 - accuracy: 0.9688 - val_loss: 0.1584 - val_accuracy: 0.9493 - lr: 5.0000e-04\n",
      "Epoch 189/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9721\n",
      "Epoch 00189: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0886 - accuracy: 0.9721 - val_loss: 0.0873 - val_accuracy: 0.9729 - lr: 5.0000e-04\n",
      "Epoch 190/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9747\n",
      "Epoch 00190: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0809 - accuracy: 0.9747 - val_loss: 0.0804 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 191/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9755\n",
      "Epoch 00191: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0781 - accuracy: 0.9755 - val_loss: 0.0792 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 192/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9755\n",
      "Epoch 00192: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0779 - accuracy: 0.9755 - val_loss: 0.0806 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 193/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9757\n",
      "Epoch 00193: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0771 - accuracy: 0.9757 - val_loss: 0.0791 - val_accuracy: 0.9755 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9755\n",
      "Epoch 00194: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0779 - accuracy: 0.9755 - val_loss: 0.0784 - val_accuracy: 0.9757 - lr: 5.0000e-04\n",
      "Epoch 195/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9760\n",
      "Epoch 00195: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0757 - accuracy: 0.9760 - val_loss: 0.0787 - val_accuracy: 0.9756 - lr: 5.0000e-04\n",
      "Epoch 196/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9755\n",
      "Epoch 00196: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.0798 - val_accuracy: 0.9752 - lr: 5.0000e-04\n",
      "Epoch 197/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9756\n",
      "Epoch 00197: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0769 - accuracy: 0.9756 - val_loss: 0.0786 - val_accuracy: 0.9755 - lr: 5.0000e-04\n",
      "Epoch 198/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9757\n",
      "Epoch 00198: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0768 - accuracy: 0.9757 - val_loss: 0.0792 - val_accuracy: 0.9750 - lr: 5.0000e-04\n",
      "Epoch 199/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9755\n",
      "Epoch 00199: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.0807 - val_accuracy: 0.9747 - lr: 5.0000e-04\n",
      "Epoch 200/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9754\n",
      "Epoch 00200: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0773 - accuracy: 0.9754 - val_loss: 0.0804 - val_accuracy: 0.9749 - lr: 5.0000e-04\n",
      "Epoch 201/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9765\n",
      "Epoch 00201: val_loss did not improve from 0.07702\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0739 - accuracy: 0.9765 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 202/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9766\n",
      "Epoch 00202: val_loss improved from 0.07702 to 0.07699, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0731 - accuracy: 0.9766 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 203/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9767\n",
      "Epoch 00203: val_loss improved from 0.07699 to 0.07695, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0731 - accuracy: 0.9767 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 204/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9765\n",
      "Epoch 00204: val_loss did not improve from 0.07695\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0735 - accuracy: 0.9765 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 205/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9766\n",
      "Epoch 00205: val_loss improved from 0.07695 to 0.07690, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0736 - accuracy: 0.9766 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 206/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9768\n",
      "Epoch 00206: val_loss improved from 0.07690 to 0.07685, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0726 - accuracy: 0.9768 - val_loss: 0.0768 - val_accuracy: 0.9761 - lr: 1.0000e-04\n",
      "Epoch 207/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9766\n",
      "Epoch 00207: val_loss did not improve from 0.07685\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0731 - accuracy: 0.9766 - val_loss: 0.0770 - val_accuracy: 0.9761 - lr: 1.0000e-04\n",
      "Epoch 208/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9767\n",
      "Epoch 00208: val_loss did not improve from 0.07685\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0727 - accuracy: 0.9767 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 209/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9767\n",
      "Epoch 00209: val_loss did not improve from 0.07685\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0730 - accuracy: 0.9767 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 210/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9767\n",
      "Epoch 00210: val_loss improved from 0.07685 to 0.07676, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 304ms/step - loss: 0.0727 - accuracy: 0.9767 - val_loss: 0.0768 - val_accuracy: 0.9761 - lr: 1.0000e-04\n",
      "Epoch 211/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9765\n",
      "Epoch 00211: val_loss did not improve from 0.07676\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0732 - accuracy: 0.9765 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 212/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9766\n",
      "Epoch 00212: val_loss did not improve from 0.07676\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0730 - accuracy: 0.9766 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 213/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9768\n",
      "Epoch 00213: val_loss did not improve from 0.07676\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0723 - accuracy: 0.9768 - val_loss: 0.0768 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 214/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9765\n",
      "Epoch 00214: val_loss did not improve from 0.07676\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0733 - accuracy: 0.9765 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 215/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9768\n",
      "Epoch 00215: val_loss improved from 0.07676 to 0.07674, saving model to best_model.h5\n",
      "25/25 [==============================] - 8s 305ms/step - loss: 0.0722 - accuracy: 0.9768 - val_loss: 0.0767 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 216/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9766\n",
      "Epoch 00216: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0730 - accuracy: 0.9766 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 217/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9768\n",
      "Epoch 00217: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0724 - accuracy: 0.9768 - val_loss: 0.0769 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 218/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9767\n",
      "Epoch 00218: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0729 - accuracy: 0.9767 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 219/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9767\n",
      "Epoch 00219: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0726 - accuracy: 0.9767 - val_loss: 0.0768 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 220/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9768\n",
      "Epoch 00220: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0722 - accuracy: 0.9768 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9767\n",
      "Epoch 00221: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0725 - accuracy: 0.9767 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 222/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9767\n",
      "Epoch 00222: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0728 - accuracy: 0.9767 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 223/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9767\n",
      "Epoch 00223: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0726 - accuracy: 0.9767 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 224/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9769\n",
      "Epoch 00224: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0719 - accuracy: 0.9769 - val_loss: 0.0769 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 225/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9770\n",
      "Epoch 00225: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0714 - accuracy: 0.9770 - val_loss: 0.0774 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 226/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9769\n",
      "Epoch 00226: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0717 - accuracy: 0.9769 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 227/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9766\n",
      "Epoch 00227: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0727 - accuracy: 0.9766 - val_loss: 0.0771 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 228/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9767\n",
      "Epoch 00228: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0726 - accuracy: 0.9767 - val_loss: 0.0773 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 229/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9769\n",
      "Epoch 00229: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0718 - accuracy: 0.9769 - val_loss: 0.0770 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 230/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9770\n",
      "Epoch 00230: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0714 - accuracy: 0.9770 - val_loss: 0.0769 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 231/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9770\n",
      "Epoch 00231: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 232/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9763\n",
      "Epoch 00232: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0733 - accuracy: 0.9763 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 233/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9769\n",
      "Epoch 00233: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0720 - accuracy: 0.9769 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 234/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9767\n",
      "Epoch 00234: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0726 - accuracy: 0.9767 - val_loss: 0.0776 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 235/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9770\n",
      "Epoch 00235: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0712 - accuracy: 0.9770 - val_loss: 0.0771 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 236/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9769\n",
      "Epoch 00236: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0718 - accuracy: 0.9769 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 237/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9769\n",
      "Epoch 00237: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0717 - accuracy: 0.9769 - val_loss: 0.0773 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 238/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9770\n",
      "Epoch 00238: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0714 - accuracy: 0.9770 - val_loss: 0.0772 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 239/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9768\n",
      "Epoch 00239: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0720 - accuracy: 0.9768 - val_loss: 0.0772 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 240/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9771\n",
      "Epoch 00240: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0707 - accuracy: 0.9771 - val_loss: 0.0775 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 241/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9769\n",
      "Epoch 00241: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0715 - accuracy: 0.9769 - val_loss: 0.0772 - val_accuracy: 0.9760 - lr: 1.0000e-04\n",
      "Epoch 242/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9770\n",
      "Epoch 00242: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0711 - accuracy: 0.9770 - val_loss: 0.0775 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 243/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9772\n",
      "Epoch 00243: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0704 - accuracy: 0.9772 - val_loss: 0.0777 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 244/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9763\n",
      "Epoch 00244: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0735 - accuracy: 0.9763 - val_loss: 0.0776 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 245/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9768\n",
      "Epoch 00245: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0718 - accuracy: 0.9768 - val_loss: 0.0776 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 246/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9769\n",
      "Epoch 00246: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0713 - accuracy: 0.9769 - val_loss: 0.0775 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 247/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9768\n",
      "Epoch 00247: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0718 - accuracy: 0.9768 - val_loss: 0.0776 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 248/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9770\n",
      "Epoch 00248: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0709 - accuracy: 0.9770 - val_loss: 0.0775 - val_accuracy: 0.9759 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9769\n",
      "Epoch 00249: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0712 - accuracy: 0.9769 - val_loss: 0.0774 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 250/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9768\n",
      "Epoch 00250: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0716 - accuracy: 0.9768 - val_loss: 0.0779 - val_accuracy: 0.9757 - lr: 1.0000e-04\n",
      "Epoch 251/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9769\n",
      "Epoch 00251: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0713 - accuracy: 0.9769 - val_loss: 0.0777 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 252/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9770\n",
      "Epoch 00252: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0710 - accuracy: 0.9770 - val_loss: 0.0778 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 253/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9770\n",
      "Epoch 00253: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0710 - accuracy: 0.9770 - val_loss: 0.0777 - val_accuracy: 0.9758 - lr: 1.0000e-04\n",
      "Epoch 254/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9771\n",
      "Epoch 00254: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0705 - accuracy: 0.9771 - val_loss: 0.0776 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 255/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9771\n",
      "Epoch 00255: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0705 - accuracy: 0.9771 - val_loss: 0.0775 - val_accuracy: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 256/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9771\n",
      "Epoch 00256: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 296ms/step - loss: 0.0704 - accuracy: 0.9771 - val_loss: 0.0777 - val_accuracy: 0.9757 - lr: 1.0000e-04\n",
      "Epoch 257/500\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9682\n",
      "Epoch 00257: val_loss did not improve from 0.07674\n",
      "25/25 [==============================] - 7s 297ms/step - loss: 0.0971 - accuracy: 0.9682 - val_loss: 0.1012 - val_accuracy: 0.9687 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, \n",
    "          validation_data=(X_validation,y_validation), callbacks=[mc, es, lrs]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "#plt.ylim((0.95,1))\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.977, Test: 0.976\n"
     ]
    }
   ],
   "source": [
    "saved_model = load_model('best_model.h5')\n",
    "#saved_model = model\n",
    "# evaluate the model\n",
    "_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = saved_model.evaluate(X_validation, y_validation, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score:  0.95467\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax((saved_model.predict(X_validation)+saved_model.predict(X_validation[:,::-1,:])[:,::-1,:])/2, axis=2).reshape(-1)\n",
    "gt = np.argmax(y_validation, axis=2).reshape(-1)\n",
    "print(\"Macro F1 score: \", np.round(f1_score(gt, pred, average=\"macro\"), 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
